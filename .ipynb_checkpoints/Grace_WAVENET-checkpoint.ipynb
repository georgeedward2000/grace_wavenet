{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *    # convert midi to numerical data and vice versa\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform musical file in numerical data\n",
    "def read_midi(file):\n",
    "    print('Loading music file: ' + str(file))\n",
    "    \n",
    "    notes = []\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    midi = converter.parse(file)\n",
    "    instruments = instrument.partitionByInstrument(midi)\n",
    "    \n",
    "    for part in instruments.parts:\n",
    "        \n",
    "        if 'Piano' in str(part):\n",
    "            \n",
    "            note_to_parse = part.recurse()\n",
    "            \n",
    "            for e in note_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(e, note.Note):\n",
    "                    notes.append(str(e.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(e, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in e.normalOrder))\n",
    "    return np.array(notes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading music file: train/gnossienne_2_(c)oguri.mid\n",
      "Loading music file: train/gnossienne_3_(c)oguri.mid\n",
      "Loading music file: train/satie_gnoissienne1.mid\n",
      "Loading music file: train/gnossienne_1_(c)oguri.mid\n",
      "Loading music file: train/gnossienne_1_(c)dery.mid\n",
      "Loading music file: train/satie_gymnopedie_no1.mid\n"
     ]
    }
   ],
   "source": [
    "# read the files names of the training music\n",
    "files = [i for i in os.listdir('train/') if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi('train/'+i) for i in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['D5', 'G2', 'E5', 'D4', '7.10', 'C5', '9.0', 'F4', 'D5', 'C5',\n",
      "       'D5', 'C5', 'D5', 'B-4', 'G2', 'C5', 'G4', 'B-3', 'D4', 'A4',\n",
      "       '9.0', 'F4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'G4', 'E2', 'B4',\n",
      "       '4.7.11', 'B3', 'G3', 'E4', 'E2', '9.0', 'F3', 'G3', 'B3', 'E4',\n",
      "       'D5', 'G2', 'E5', 'G4', 'D4', 'B-3', 'C5', 'F4', '9.0', 'D5', 'C5',\n",
      "       'D5', 'C5', 'D5', 'G2', 'B-4', 'C5', 'G4', 'B-3', 'D4', 'F4',\n",
      "       '9.0', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'G#4', 'D2', 'F4',\n",
      "       'D4', '5.8', 'G#3', 'D4', 'F3', 'D2', 'B3', 'G3', 'E3', 'G#3',\n",
      "       'D4', 'F3', 'D2', 'F4', 'G4', 'G#4', '2.8', 'F3', 'B-4', 'G#4',\n",
      "       'G4', 'F4', 'G4', 'D4', 'G#4', 'G#3', 'F3', 'B-4', 'G#4', 'E2',\n",
      "       'B4', 'B4', 'E4', '7.11', 'G3', 'B3', 'E4', 'D2', 'F4', 'G4',\n",
      "       'G#4', '5.8', '2.5', 'B-4', 'C5', 'D5', 'C5', 'B-4', 'G#4', '5.8',\n",
      "       'D4', 'B-4', 'G#4', 'B4', 'E2', 'B4', 'B3', 'G3', 'E4', 'G3', 'B3',\n",
      "       'E4', 'B4', 'A2', 'A4', 'C#4', 'E4', 'A3', 'A3', 'C#4', 'E4', 'A2',\n",
      "       'B3', 'G3', 'D4', 'A3', 'C#4', 'E4', 'A2', 'E5', 'E-5', '9.1',\n",
      "       'E4', 'C#5', 'E-5', 'E5', 'F#5', 'G#5', 'F#5', 'E5', '9.1', 'E4',\n",
      "       'E-5', 'C#5', 'F#2', 'B-4', 'C#5', 'B-4', 'C#4', '6.10', 'B-3',\n",
      "       'F#4', 'C#4', 'B4', 'F2', 'C5', 'C5', '5.9.0', 'C4', 'A3', 'F4',\n",
      "       'A2', 'E5', 'E-5', 'C#5', '9.1', 'E4', 'E-5', 'E5', 'F#5', 'G#5',\n",
      "       'F#5', 'E5', '9.1', 'E4', 'E-5', 'C#5', 'F#2', 'B-4', 'C#5', 'B-4',\n",
      "       '6.10', 'C#4', 'B-3', 'F#4', 'C#4', 'B4', 'C5', 'F2', 'C5', '0.5',\n",
      "       'A3', '9.0', 'F4', 'C5', 'A4', 'D2', 'A4', '9.2', 'F4', 'F4', 'A3',\n",
      "       'D4', 'D2', 'E-4', 'G3', 'C4', 'F4', 'A3', 'D4', 'D2', 'F4', 'G4',\n",
      "       'G#4', 'D4', 'G#3', 'F3', 'B-4', 'G#4', 'G4', 'F4', 'G4', 'G#4',\n",
      "       'G#3', 'D4', 'F3', 'B-4', 'G#4', 'E2', 'B4', 'B4', '7.11', 'E4',\n",
      "       'E4', 'G3', 'B3', 'D2', 'F4', 'G4', 'G#4', '2.8', 'F3', 'B-4',\n",
      "       'C5', 'D5', 'C5', 'B-4', 'G#4', '5.8', 'D4', 'B-4', 'G#4', 'B4',\n",
      "       'E2', 'B4', '4.7.11', 'G3', 'B3', 'E4', 'B4', 'A2', 'A4', 'C#4',\n",
      "       'A3', 'E4', 'C#4', 'A3', 'E4', 'C3', 'E5', '0.4.7', 'G3', 'E4',\n",
      "       'C4', 'G2', 'D5', 'E5', '7.10', 'D4', 'C5', 'F4', '9.0', 'D5',\n",
      "       'C5', 'D5', 'C5', 'D5', 'G2', 'B-4', 'C5', 'G4', 'B-3', 'D4', 'A4',\n",
      "       '5.9', 'C4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'E2', 'G4', 'B4',\n",
      "       '7.11', 'E4', 'B3', 'G3', 'E4', 'E2', 'A3', 'C4', 'F3', '7.11',\n",
      "       'E4'], dtype='<U6')\n",
      " array(['A2', 'A5', 'A5', '4.9', 'E-5', 'C4', 'E-5', 'F#5', 'F#5', '9.0.4',\n",
      "       'A2', '4.9', 'E5', 'C4', 'E5', 'E-5', 'E-5', '0.4', 'A4', 'C4',\n",
      "       'C5', '11.4', 'B4', '4.7.11', '7.11', 'E4', 'E2', 'B3', '4.7',\n",
      "       '7.11', 'E4', 'A2', 'A5', 'A5', 'E-5', '9.0.4', 'E-5', 'F#5',\n",
      "       'F#5', '9.0.4', 'A2', 'E5', '9.0', 'E4', 'E5', 'E-5', 'E-5', '0',\n",
      "       'A4', '0.4', 'C5', '11.4', 'B4', '4.7.11', '4.7.11', 'B4', 'D2',\n",
      "       'D5', 'B4', '2.5.9', 'B4', 'C5', 'C5', 'F4', 'A3', 'D4', '11.4',\n",
      "       'B4', '11.4', 'G#3', '8.11', 'E4', 'B4', 'D2', 'D5', '9.2', 'F4',\n",
      "       '2.5.9', 'D2', 'B4', '2.5.9', 'B4', 'D5', 'D5', 'B4', '2.5', 'A3',\n",
      "       'B4', 'G#4', 'D2', 'A4', 'B4', '9.2', 'F4', 'D5', 'B4', 'A4',\n",
      "       'G#4', 'F4', 'D4', 'A3', 'A4', 'B4', 'D2', 'D5', 'B4', '2.5.9',\n",
      "       'A4', 'G#4', 'A4', 'F4', 'B4', 'D4', 'A3', 'D5', 'A1', 'F5', 'F5',\n",
      "       'A4', '9.0.4', 'A4', '9.0.4', 'A2', '4.9', 'C4', 'A3', 'E4', 'C4',\n",
      "       '4', 'E5', 'B-4', '4.7.11', 'B-4', 'C#5', 'C#5', 'G3', '11.4',\n",
      "       'E2', 'B4', '4.7.11', 'B4', 'B-4', 'B-4', 'G4', '11.4', 'G3', 'G4',\n",
      "       '6.11', 'F#4', '2.6', 'B3', '2.6', 'B3', 'B1', 'F#3', 'B3', 'D4',\n",
      "       'F#3', 'B3', 'D4', 'E2', 'E5', 'E5', 'B-4', '4.7.11', 'B-4', 'C#5',\n",
      "       'C#5', 'B3', '4.7', 'E2', 'B4', '7.11', 'E4', 'B4', 'B-4', 'B-4',\n",
      "       '4.7.11', 'G3', 'G4', 'F#4', 'B1', 'F#4', '6.11', 'D4', '6.11',\n",
      "       '11.2', 'F#4', 'F#1', 'A4', '6.9', 'C#4', '1.6', 'A3', 'E2', 'E4',\n",
      "       'F#4', 'G4', '4.7.11', 'B-4', 'B4', 'C#5', '4.7.11', 'E4', 'C#5',\n",
      "       'B4', 'E2', 'B-4', 'G4', '7.11', 'E4', 'F#4', 'E4', 'F#4', 'B3',\n",
      "       '7', 'E4', 'B-4', 'B4', 'E2', 'C#5', '11.4', 'E5', 'G3', 'C#5',\n",
      "       'B4', 'B-4', 'G4', '11.4', 'G3', 'F#4', 'E4', 'E2', 'F#4',\n",
      "       '4.7.11', 'G4', 'B-4', 'B4', 'C#5', 'G3', '11.4', 'E5', 'C#5',\n",
      "       '11.2', 'D5', '11.2.6', '11.2.6', 'B1', '11.2', 'F#3', 'F#3', 'B3',\n",
      "       'D4', 'E2', 'E4', 'F#4', 'G4', '7.11', 'E4', 'B-4', 'B4', 'C#5',\n",
      "       'E5', '11.4', 'G3', 'C#5', 'E2', 'B4', 'B-4', 'B3', 'E4', 'G3',\n",
      "       'G4', 'F#4', 'E4', 'F#4', '7', 'E4', 'B3', 'B-4', 'B4', 'E2',\n",
      "       'C#5', '11.4', 'G3', 'E5', 'C#5', 'B4', 'B-4', 'G4', '11.4', 'G3',\n",
      "       'F#4', 'E2', 'E4', 'F#4', '4.7.11', 'G4', 'B-4', 'B4', 'C#5',\n",
      "       '11.4', 'G3', 'E5', 'C#5', 'D5', 'B1', 'D5', 'B3', '2.6', '11.2.6',\n",
      "       'A1', 'F#4', 'A4', 'E4', 'C4', 'A3', '0.4', 'A3', 'A2', '4.9',\n",
      "       '9.0', '9.0', 'E4', 'C5', 'F2', 'G#4', '5.8.0', 'G#3', 'C4', 'F4',\n",
      "       'F2', 'G#3', '0.5', 'G#3', 'F4', 'C4', 'F2', 'G#4', 'B4', '5.8.0',\n",
      "       'C5', 'E-5', 'D5', 'C5', 'G#3', 'B4', '0.5', 'G#4', 'F2', 'C5',\n",
      "       'C5', '5.8', 'C4', '5.8', 'C4', 'B4', 'D2', 'D5', 'F4', 'A3', 'D4',\n",
      "       'F4', '9.2', 'D2', 'F4', '9.2', '2.5', 'A3', 'F2', 'G#4', 'B4',\n",
      "       '5.8', 'C5', 'C4', 'E-5', 'D5', 'C5', 'B4', 'G#3', '0.5', 'G#4',\n",
      "       'F2', 'C5', 'C5', '5.8', 'C4', '5.8', 'C4', 'B4', 'D2', 'D5', 'F4',\n",
      "       'D4', 'A3', 'F4', '9.2', 'A2', 'A5', 'A5', 'E-5', '4.9', 'C4',\n",
      "       'E-5', 'F#5', 'F#5', '9.0.4', 'A2', '4.9', 'E5', '0.4', 'E5',\n",
      "       'E-5', 'E-5', '9.0', 'C4', 'E4', 'C5', 'B4', 'E2', 'B4', 'B3',\n",
      "       '4.7', '4.7.11', 'E2', '4.7.11', '4.7.11', 'A2', 'A5', 'A5', 'E-5',\n",
      "       'A4', '0.4', 'E-5', 'F#5', 'F#5', 'A4', 'E4', 'C4', 'A2', 'E5',\n",
      "       '4.9', '0.4', 'E5', 'E-5', 'E-5', '0.4', 'C4', 'A4', 'C5', '11.4',\n",
      "       'B4', '4.7.11', 'B4', 'G3', '11.4', 'B4', 'A1', 'E5', 'E5', 'E4',\n",
      "       'A3', 'C4', '0.4', 'A3', 'A2', '4.9', 'C4', '9.0', 'E4'],\n",
      "      dtype='<U6')\n",
      " array(['F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', 'B4', 'F2',\n",
      "       '5.8.0', 'C5', 'B4', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5',\n",
      "       'C5', '5.8.0', 'E5', 'F5', 'F2', '5.8.0', 'E5', 'F5', '5.8.0',\n",
      "       'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', 'B4', 'F2',\n",
      "       '5.8.0', 'G#4', 'G4', '8.0', 'F4', 'G4', 'C3', 'F4', 'G4', '0.3.7',\n",
      "       '0.3.7', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2',\n",
      "       '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0',\n",
      "       'C5', 'B4', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'F2', 'C5',\n",
      "       '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'E5', 'F5', 'F2', '5.8.0',\n",
      "       'E5', 'F5', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5',\n",
      "       '5.8.0', 'C5', 'B4', 'F2', '5.8.0', 'G#4', 'G4', '8.0', 'F4', 'G4',\n",
      "       'C3', 'F4', 'G4', '0.3.7', '0.3.7', 'G#4', 'G4', 'F2', 'G4', 'F4',\n",
      "       '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1',\n",
      "       '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5',\n",
      "       'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0',\n",
      "       '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5',\n",
      "       'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4',\n",
      "       '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2',\n",
      "       '5.8.0', '5.8.0', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5',\n",
      "       'B5', '5.8.0', 'G5', 'F5', 'F2', 'G5', 'F5', '5.8.0', '5.8.0',\n",
      "       'G5', 'F5', 'F2', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5', 'C5', 'F2',\n",
      "       '5.8.0', 'C5', 'B4', '5.8.0', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0',\n",
      "       '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2', 'C5', 'D5', '5.8.0', 'E5',\n",
      "       'F5', 'G5', 'B5', '5.8.0', 'G5', 'F5', 'F2', 'G5', 'F5', '5.8.0',\n",
      "       '5.8.0', 'G5', 'F5', 'F2', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5',\n",
      "       'C5', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'G#4', 'G4', 'F2', 'G4',\n",
      "       'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1',\n",
      "       '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5',\n",
      "       'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0',\n",
      "       '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5',\n",
      "       'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4',\n",
      "       '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2',\n",
      "       '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0',\n",
      "       'C5', '5.11', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5',\n",
      "       'C5', '5.8.0', 'E5', '5', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0',\n",
      "       'E-5', 'D5', 'C5', '5.8.0', 'C5', '5.11', '5.8.0', '5.8.0', 'F2',\n",
      "       'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'E5', 'F5', 'F2',\n",
      "       '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4',\n",
      "       '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5',\n",
      "       'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0',\n",
      "       '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5',\n",
      "       'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4',\n",
      "       'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2',\n",
      "       'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5', 'B5', '5.8.0', 'G5', 'F5',\n",
      "       'F2', 'G5', 'F5', '5.8.0', '5.8.0', 'G5', 'F5', 'F2', '5.8.0',\n",
      "       'F5', 'E5', '5.8.0', 'C#5', 'C5', 'F2', '5.8.0', 'C5', 'B4',\n",
      "       '5.8.0', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2',\n",
      "       '5.8.0', '5.8.0', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5',\n",
      "       'B5', '5.8.0', 'G5', 'F5', 'F2', 'G5', 'F5', '5.8.0', '5.8.0',\n",
      "       'G5', 'F5', 'F2', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5', 'C5', 'F2',\n",
      "       '5.8.0', 'C5', 'B4', '5.8.0', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0',\n",
      "       '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5',\n",
      "       'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4',\n",
      "       '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2',\n",
      "       '5.8.0', '5.8.0'], dtype='<U6')\n",
      " array(['F2', '8.0', 'C5', '5.8', 'E-5', 'D5', 'C5', '0.5', 'G#3', 'C5',\n",
      "       'B4', 'F2', '8.0', 'F4', 'C5', 'B4', '5.8.0', 'F2', '5.8.0', 'C5',\n",
      "       'E-5', 'D5', 'G#3', 'C5', 'C4', 'F4', 'E5', 'F2', 'F5', '5.8.0',\n",
      "       'E5', 'F5', '8.0', 'F4', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5',\n",
      "       '0.5', 'G#3', 'C5', 'F2', 'B4', '5.8', 'C4', 'G#4', 'G4', 'G#3',\n",
      "       '0.5', 'F4', 'C3', 'G4', 'F4', '7.0', 'E-3', 'G4', 'G3', 'E-3',\n",
      "       'C4', 'F2', 'G#4', 'G4', 'G4', '8.0', 'F4', 'F4', 'G#3', 'C4',\n",
      "       'F2', '5.8', 'C4', '8.0', 'F4', 'F2', '5.8.0', 'C5', 'E-5', 'D5',\n",
      "       'C5', '8.0', 'F4', 'C5', 'B4', 'F2', '5.8.0', 'C5', 'B4', '5.8',\n",
      "       '0.5', 'F2', '5.8.0', 'C5', 'E-5', 'D5', 'C5', 'C4', 'F4', 'G#3',\n",
      "       'E5', 'F2', 'F5', '5.8.0', 'E5', 'F5', '8.0', 'F4', 'F2', '8.0',\n",
      "       'C5', 'F4', 'E-5', 'D5', 'C5', 'G#3', '0.5', 'C5', 'F2', 'B4',\n",
      "       '5.8.0', 'G#4', 'G4', '5.8', '0.5', 'F4', 'C3', 'G4', '7.0', 'F4',\n",
      "       'E-3', 'G4', 'C4', 'G3', 'E-3', 'G#4', 'F2', 'G4', 'G4', '8.0',\n",
      "       'F4', 'F4', 'G#3', 'C4', 'F2', '8.0', 'F4', '5.8', 'C4', 'G#4',\n",
      "       'B-1', 'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'B-4',\n",
      "       'B-1', '10.1.5', 'G#4', 'G4', 'C#4', 'B-3', 'F3', 'G#4', 'F2',\n",
      "       'G4', 'G4', 'F4', '8.0', 'G#3', 'C4', 'F4', 'F2', 'F4', '8.0',\n",
      "       'F4', 'G#3', 'C4', 'G#4', 'B-1', 'B-4', '10.1.5', 'G#4', 'G4',\n",
      "       '10.1', 'F3', 'B-1', 'G#4', 'B-4', '10.1.5', 'G#4', 'G4', '10.1',\n",
      "       'F3', 'G#4', 'G4', 'F2', 'G4', '5.8.0', 'F4', 'G#3', 'C4', 'F2',\n",
      "       'F4', '8.0', 'G#3', '0.5', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5',\n",
      "       'G5', 'B5', '0.5', 'G#3', 'G5', 'F5', 'F2', 'G5', '5.8.0', 'F5',\n",
      "       '0.5', 'G#3', 'G5', 'F2', 'F5', '5.8.0', 'F5', 'E5', '0.5', 'G#3',\n",
      "       'C#5', 'F2', 'C5', '5.8.0', 'C5', 'B4', 'G#3', '0.5', 'G#4', 'F2',\n",
      "       'G4', 'G4', 'F4', 'G#3', 'C4', 'F4', 'G#3', 'C4', 'F2', 'F4',\n",
      "       'G#3', 'C4', 'F4', 'G#3', 'C4', 'F2', 'C5', '5.8.0', 'D5', 'E5',\n",
      "       'F5', 'G5', 'G#3', 'B5', '0.5', 'G5', 'F5', 'F2', 'G5', '5.8',\n",
      "       'C4', 'F5', '5.8', '0.5', 'F2', 'G5', 'F5', '5.8', 'C4', 'F5',\n",
      "       'E5', '5.8', '0.5', 'C#5', 'F2', 'C5', 'G#3', '0.5', 'C5', 'B4',\n",
      "       '5.8.0', 'G#4', 'F2', 'G4', 'G4', 'C4', 'G#3', 'F4', 'G#3', 'C4',\n",
      "       'F4', 'F2', '8.0', 'F4', 'G#3', 'F4', 'C4', 'B-1', 'G#4', 'B-4',\n",
      "       '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-1', 'B-4', '10.1.5',\n",
      "       'G#4', 'G4', 'B-3', 'C#4', 'F3', 'F2', 'G#4', 'G4', 'G4', 'C4',\n",
      "       'G#3', 'F4', '5.8.0', 'F2', '5.8.0', '8.0', '0.5', 'G#4', 'B-1',\n",
      "       'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'B-1', 'G#4',\n",
      "       'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'F2', 'G4',\n",
      "       'G4', '5.8.0', '5.8.0', 'F2', 'G#3', 'F4', 'C4', 'G#3', 'C4', 'F4',\n",
      "       'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', 'F4', 'G#3', 'C4', 'C5',\n",
      "       'F2', 'B4', '5.8.0', 'G#3', '0.5', 'F2', 'C5', '5.8.0', 'E-5',\n",
      "       'D5', 'C5', 'F4', 'G#3', 'C4', 'E5', 'F2', 'F5', '5.8.0', '8.0',\n",
      "       'F4', 'F2', '8.0', 'C5', 'F4', 'E-5', 'D5', '5.8', 'C5', 'C4',\n",
      "       'C5', 'F2', 'B4', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5',\n",
      "       'D5', 'G#3', 'C5', '0.5', 'E5', 'F2', 'F5', 'G#3', 'F4', 'C4',\n",
      "       '5.8', 'C4', 'B-1', 'G#4', 'B-4', '10.1', 'F3', 'G#4', 'G4',\n",
      "       '10.1', 'F3', 'B-1', 'G#4', 'B-4', '10.1.5', 'G#4', 'G4', '10.1',\n",
      "       'F3', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', '5.8', 'C4', 'F2',\n",
      "       '5.8.0', '5.8', 'C4', 'B-1', 'G#4', 'B-4', '10.1', 'F3', 'G#4',\n",
      "       'G4', '10.1', 'F3', 'G#4', 'B-1', 'B-4', '10.1.5', 'G#4', 'G4',\n",
      "       '10.1', 'F3', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', 'F4', '8.0',\n",
      "       'F2', 'G#3', 'F4', 'C4', 'G#3', 'F4', 'C4', 'F2', 'C5', 'D5',\n",
      "       '5.8', 'C4', 'E5', 'F5', 'G5', 'G#3', '0.5', 'B5', 'G5', 'F5',\n",
      "       'F2', 'G5', '5.8.0', 'F5', '5.8', 'C4', 'F2', 'G5', 'F5', '5.8.0',\n",
      "       'F5', 'E5', '5.8.0', 'C#5', 'F2', 'C5', '5.8.0', 'C5', 'B4', 'G#3',\n",
      "       '0.5', 'G#4', 'F2', 'G4', '8.0', 'G4', 'F4', 'F4', 'G#3', 'C4',\n",
      "       'F2', '0.5', 'G#3', 'F4', 'G#3', 'C4', 'F2', 'C5', '5.8', 'D5',\n",
      "       'C4', 'E5', 'F5', 'G5', 'G#3', '0.5', 'B5', 'G5', 'F2', 'F5',\n",
      "       '5.8.0', 'G5', 'F5', '5.8.0', 'F2', 'G5', 'F5', '5.8', 'C4', 'F5',\n",
      "       'E5', '5.8.0', 'C#5', 'F2', 'C5', '5.8.0', 'C5', 'B4', 'G#3',\n",
      "       '0.5', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', 'G#3', 'F4', 'C4',\n",
      "       'F2', '5.8.0', 'G#3', 'F4', 'C4', 'B-1', 'G#4', 'B-4', '10.1',\n",
      "       'F3', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'B-1', 'B-4', '10.1.5',\n",
      "       'G#4', 'G4', 'C#4', 'B-3', 'F3', 'G#4', 'F2', 'G4', 'G4', '8.0',\n",
      "       'F4', 'F4', '8.0', 'F2', '5.8.0', 'F4', 'G#3', 'C4', 'B-1', 'G#4',\n",
      "       'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', '8.10', 'B-4', '10.1',\n",
      "       'F3', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'F2', 'G4', 'G4', 'G#3',\n",
      "       'C4', 'F4'], dtype='<U6')\n",
      " array(['F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4',\n",
      "       'F2', '0.5', 'G#3', 'C5', 'B4', '0.5', 'G#3', 'F2', '0.5', 'C5',\n",
      "       'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2', '0.5', 'G#3',\n",
      "       'E5', 'F5', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5',\n",
      "       '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', 'G#4', 'G4', 'F4',\n",
      "       'C4', 'G#3', 'G4', 'C3', 'F4', 'G4', '0.3.7', '0.3.7', 'G#4', 'G4',\n",
      "       'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3',\n",
      "       'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4',\n",
      "       'F2', '0.5', 'G#3', 'C5', 'B4', '0.5', 'G#3', 'F2', '0.5', 'C5',\n",
      "       'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2', '0.5', 'G#3',\n",
      "       'E5', 'F5', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5',\n",
      "       '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', 'G#4', 'G4', 'F4',\n",
      "       'C4', 'G#3', 'G4', 'C3', 'F4', 'G4', '0.3.7', '0.3.7', 'G#4', 'G4',\n",
      "       'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3',\n",
      "       'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10',\n",
      "       'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10',\n",
      "       'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3',\n",
      "       '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4',\n",
      "       'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4',\n",
      "       'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2',\n",
      "       '0.5', 'G#3', '0.5', 'G#3', 'F2', 'C5', '0.5', 'D5', 'G#3', 'E5',\n",
      "       'F5', 'G5', '0.5', 'B5', 'G#3', 'G5', 'F5', 'F2', 'G5', '0.5',\n",
      "       'F5', 'G#3', '0.5', 'G#3', 'G5', 'F5', 'F2', '0.5', 'G#3', 'E5',\n",
      "       '0.5', 'G#3', 'C#5', 'C5', 'F2', '0.5', 'G#3', 'B4', '0.5', 'G#3',\n",
      "       'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3',\n",
      "       '0.5', 'G#3', 'F2', 'C5', '0.5', 'D5', 'G#3', 'E5', 'F5', 'G5',\n",
      "       '0.5', 'B5', 'G#3', 'G5', 'F5', 'F2', 'G5', '0.5', 'F5', 'G#3',\n",
      "       '0.5', 'G#3', 'G5', 'F5', 'F2', '0.5', 'G#3', 'E5', '0.5', 'G#3',\n",
      "       'C#5', 'C5', 'F2', '0.5', 'G#3', 'B4', '0.5', 'G#3', 'G#4', 'G4',\n",
      "       'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3',\n",
      "       'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10',\n",
      "       'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10',\n",
      "       'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3',\n",
      "       '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4',\n",
      "       'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4',\n",
      "       'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2',\n",
      "       '0.5', 'G#3', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5',\n",
      "       '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2',\n",
      "       '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2',\n",
      "       '0.5', 'G#3', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5',\n",
      "       '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2',\n",
      "       '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2',\n",
      "       '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10',\n",
      "       'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10',\n",
      "       'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4',\n",
      "       'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1',\n",
      "       'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1',\n",
      "       'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2',\n",
      "       '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2',\n",
      "       'C5', '0.5', 'D5', 'G#3', 'E5', 'F5', 'G5', '0.5', 'B5', 'G#3',\n",
      "       'G5', 'F5', 'F2', 'G5', '0.5', 'F5', 'G#3', '0.5', 'G#3', 'G5',\n",
      "       'F5', 'F2', '0.5', 'G#3', 'E5', '0.5', 'G#3', 'C#5', 'C5', 'F2',\n",
      "       '0.5', 'G#3', 'B4', '0.5', 'G#3', 'G#4', 'G4', 'F2', '0.5', 'G#3',\n",
      "       'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', 'C5', '0.5',\n",
      "       'D5', 'G#3', 'E5', 'F5', 'G5', '0.5', 'B5', 'G#3', 'G5', 'F5',\n",
      "       'F2', 'G5', '0.5', 'F5', 'G#3', '0.5', 'G#3', 'G5', 'F5', 'F2',\n",
      "       '0.5', 'G#3', 'E5', '0.5', 'G#3', 'C#5', 'C5', 'F2', '0.5', 'G#3',\n",
      "       'B4', '0.5', 'G#3', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3',\n",
      "       'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4',\n",
      "       '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4',\n",
      "       '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5',\n",
      "       'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4',\n",
      "       'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4',\n",
      "       'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4',\n",
      "       'F2', '0.5', 'G#3'], dtype='<U5')\n",
      " array(['G2', '11.2.6', 'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2',\n",
      "       'F#5', '11.2.6', 'A5', 'G5', 'D2', 'F#5', '6.9.1', 'C#5', 'B4',\n",
      "       'G2', 'C#5', '11.2.6', 'D5', 'A4', 'D2', '6.9.1', 'G2', '11.2.6',\n",
      "       'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', 'F#5',\n",
      "       '11.2.6', 'A5', 'G5', 'D2', 'F#5', '6.9.1', 'C#5', 'B4', 'G2',\n",
      "       'C#5', '11.2.6', 'D5', '9.2', '6.9.1', '1.6', '6.9.1', '6.11',\n",
      "       '11.2.6', 'E4', 'E2', '7.11', 'E2', '7.11.2', 'D2', '2.5.9', 'A4',\n",
      "       'A1', 'B4', '9.0.4', 'C5', 'E5', 'D2', 'D5', '4.7.11', 'B4', 'D5',\n",
      "       'D2', 'C5', '11.2.4.7', 'B4', 'D5', 'D2', '9.0.2.4', 'D2',\n",
      "       '6.9.0.2', 'D5', 'E5', 'D2', 'F5', '5.9.0', 'G5', 'A5', 'D2', 'C5',\n",
      "       '9.0.4', 'D5', 'E5', 'D2', 'D5', '11.2.4.7', 'B4', 'D5', 'D2',\n",
      "       '9.0.2.4', 'D2', '6.9.0.2', 'D5', '4.7', '4.7.11', '6', '6.9.1',\n",
      "       'B4', 'B1', 'A4', '11.2.6', 'B4', 'C#5', 'E2', 'D5', '9.1.4', 'E5',\n",
      "       'C#5', 'E2', 'D5', '6.9.1', 'E5', 'E2', '9.11.2', '11.2.4.7', 'C5',\n",
      "       '4.7.9.0', '2.6.9', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', '11.2.6',\n",
      "       'D2', '6.9.1', 'G2', 'F#5', '11.2.6', 'A5', 'G5', 'D2', 'F#5',\n",
      "       '6.9.1', 'C#5', 'B4', 'G2', 'C#5', '11.2.6', 'D5', 'A4', 'D2',\n",
      "       '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', '11.2.6', 'D2',\n",
      "       '6.9.1', 'G2', 'F#5', '11.2.6', 'A5', 'G5', 'D2', 'F#5', '6.9.1',\n",
      "       'C#5', 'B4', 'G2', 'C#5', '11.2.6', 'D5', '9.2', '6.9.1', '1.6',\n",
      "       '6.9.1', '6.11', '11.2.6', 'E4', 'E2', '7.11', 'E2', '7.11.2',\n",
      "       'D2', '2.5.9', 'A4', 'A1', 'B4', '9.0.4', 'C5', 'E5', 'D2', 'D5',\n",
      "       '4.7.11', 'B4', 'D5', 'D2', 'C5', '11.2.4.7', 'B4', 'D5', 'D2',\n",
      "       '9.0.2.4', 'D2', '6.9.0.2', 'D5', 'E5', 'D2', 'F5', '5.9.0', 'G5',\n",
      "       'A5', 'D2', 'C5', '9.0.4', 'D5', 'E5', 'D2', 'D5', '11.2.4.7',\n",
      "       'B4', 'D5', 'D2', '9.0.2.4', 'D2', '6.9.0.2', 'D5', 'G5', 'E2',\n",
      "       '4.7.11', 'F5', 'E2', '2.5.9', 'B4', 'E2', 'C5', '5.9.0', 'F5',\n",
      "       'E5', 'E2', 'D5', '9.0.4', 'C5', 'E5', 'E2', 'D5', '5.9.0', 'C5',\n",
      "       'E2', '9.11.2', '11.2.4.7', 'C5', '4.7.9.0', '2.5.9'], dtype='<U8')]\n"
     ]
    }
   ],
   "source": [
    "print(notes_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2D array into 1D array\n",
    "notes = [e for n in notes_array for e in n]\n",
    "\n",
    "#Number of unique notes\n",
    "unique_notes = list(set(notes))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([58., 14.,  7.,  0.,  5.,  2.,  0.,  3.,  2.,  2.]),\n",
       " array([  1. ,  20.2,  39.4,  58.6,  77.8,  97. , 116.2, 135.4, 154.6,\n",
       "        173.8, 193. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 319
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq = dict(Counter(notes))\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=5]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['D5', 'G2', 'E5', 'D4', 'C5', '9.0', 'F4', 'D5', 'C5', 'D5', 'C5', 'D5', 'B-4', 'G2', 'C5', 'G4', 'B-3', 'D4', 'A4', '9.0', 'F4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'G4', 'E2', 'B4', '4.7.11', 'B3', 'G3', 'E4', 'E2', '9.0', 'F3', 'G3', 'B3', 'E4', 'D5', 'G2', 'E5', 'G4', 'D4', 'B-3', 'C5', 'F4', '9.0', 'D5', 'C5', 'D5', 'C5', 'D5', 'G2', 'B-4', 'C5', 'G4', 'B-3', 'D4', 'F4', '9.0', 'A4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'G#4', 'D2', 'F4', 'D4', '5.8', 'G#3', 'D4', 'F3', 'D2', 'B3', 'G3', 'G#3', 'D4', 'F3', 'D2', 'F4', 'G4', 'G#4', 'F3', 'B-4', 'G#4', 'G4', 'F4', 'G4', 'D4', 'G#4', 'G#3', 'F3', 'B-4', 'G#4', 'E2', 'B4', 'B4', 'E4', '7.11', 'G3', 'B3', 'E4', 'D2', 'F4', 'G4', 'G#4', '5.8', 'B-4', 'C5', 'D5', 'C5', 'B-4', 'G#4', '5.8', 'D4', 'B-4', 'G#4', 'B4', 'E2', 'B4', 'B3', 'G3', 'E4', 'G3', 'B3', 'E4', 'B4', 'A2', 'A4', 'C#4', 'E4', 'A3', 'A3', 'C#4', 'E4', 'A2', 'B3', 'G3', 'D4', 'A3', 'C#4', 'E4', 'A2', 'E5', 'E-5', 'E4', 'C#5', 'E-5', 'E5', 'F#5', 'F#5', 'E5', 'E4', 'E-5', 'C#5', 'B-4', 'C#5', 'B-4', 'C#4', 'B-3', 'F#4', 'C#4', 'B4', 'F2', 'C5', 'C5', '5.9.0', 'C4', 'A3', 'F4', 'A2', 'E5', 'E-5', 'C#5', 'E4', 'E-5', 'E5', 'F#5', 'F#5', 'E5', 'E4', 'E-5', 'C#5', 'B-4', 'C#5', 'B-4', 'C#4', 'B-3', 'F#4', 'C#4', 'B4', 'C5', 'F2', 'C5', '0.5', 'A3', '9.0', 'F4', 'C5', 'A4', 'D2', 'A4', '9.2', 'F4', 'F4', 'A3', 'D4', 'D2', 'G3', 'C4', 'F4', 'A3', 'D4', 'D2', 'F4', 'G4', 'G#4', 'D4', 'G#3', 'F3', 'B-4', 'G#4', 'G4', 'F4', 'G4', 'G#4', 'G#3', 'D4', 'F3', 'B-4', 'G#4', 'E2', 'B4', 'B4', '7.11', 'E4', 'E4', 'G3', 'B3', 'D2', 'F4', 'G4', 'G#4', 'F3', 'B-4', 'C5', 'D5', 'C5', 'B-4', 'G#4', '5.8', 'D4', 'B-4', 'G#4', 'B4', 'E2', 'B4', '4.7.11', 'G3', 'B3', 'E4', 'B4', 'A2', 'A4', 'C#4', 'A3', 'E4', 'C#4', 'A3', 'E4', 'C3', 'E5', 'G3', 'E4', 'C4', 'G2', 'D5', 'E5', 'D4', 'C5', 'F4', '9.0', 'D5', 'C5', 'D5', 'C5', 'D5', 'G2', 'B-4', 'C5', 'G4', 'B-3', 'D4', 'A4', 'C4', 'B-4', 'A4', 'B-4', 'A4', 'B-4', 'E2', 'G4', 'B4', '7.11', 'E4', 'B3', 'G3', 'E4', 'E2', 'A3', 'C4', 'F3', '7.11', 'E4'])\n",
      " list(['A2', 'A5', 'A5', '4.9', 'E-5', 'C4', 'E-5', 'F#5', 'F#5', '9.0.4', 'A2', '4.9', 'E5', 'C4', 'E5', 'E-5', 'E-5', '0.4', 'A4', 'C4', 'C5', '11.4', 'B4', '4.7.11', '7.11', 'E4', 'E2', 'B3', '7.11', 'E4', 'A2', 'A5', 'A5', 'E-5', '9.0.4', 'E-5', 'F#5', 'F#5', '9.0.4', 'A2', 'E5', '9.0', 'E4', 'E5', 'E-5', 'E-5', 'A4', '0.4', 'C5', '11.4', 'B4', '4.7.11', '4.7.11', 'B4', 'D2', 'D5', 'B4', '2.5.9', 'B4', 'C5', 'C5', 'F4', 'A3', 'D4', '11.4', 'B4', '11.4', 'G#3', 'E4', 'B4', 'D2', 'D5', '9.2', 'F4', '2.5.9', 'D2', 'B4', '2.5.9', 'B4', 'D5', 'D5', 'B4', 'A3', 'B4', 'G#4', 'D2', 'A4', 'B4', '9.2', 'F4', 'D5', 'B4', 'A4', 'G#4', 'F4', 'D4', 'A3', 'A4', 'B4', 'D2', 'D5', 'B4', '2.5.9', 'A4', 'G#4', 'A4', 'F4', 'B4', 'D4', 'A3', 'D5', 'A1', 'F5', 'F5', 'A4', '9.0.4', 'A4', '9.0.4', 'A2', '4.9', 'C4', 'A3', 'E4', 'C4', 'E5', 'B-4', '4.7.11', 'B-4', 'C#5', 'C#5', 'G3', '11.4', 'E2', 'B4', '4.7.11', 'B4', 'B-4', 'B-4', 'G4', '11.4', 'G3', 'G4', '6.11', 'F#4', 'B3', 'B3', 'B1', 'B3', 'D4', 'B3', 'D4', 'E2', 'E5', 'E5', 'B-4', '4.7.11', 'B-4', 'C#5', 'C#5', 'B3', 'E2', 'B4', '7.11', 'E4', 'B4', 'B-4', 'B-4', '4.7.11', 'G3', 'G4', 'F#4', 'B1', 'F#4', '6.11', 'D4', '6.11', 'F#4', 'A4', 'C#4', 'A3', 'E2', 'E4', 'F#4', 'G4', '4.7.11', 'B-4', 'B4', 'C#5', '4.7.11', 'E4', 'C#5', 'B4', 'E2', 'B-4', 'G4', '7.11', 'E4', 'F#4', 'E4', 'F#4', 'B3', 'E4', 'B-4', 'B4', 'E2', 'C#5', '11.4', 'E5', 'G3', 'C#5', 'B4', 'B-4', 'G4', '11.4', 'G3', 'F#4', 'E4', 'E2', 'F#4', '4.7.11', 'G4', 'B-4', 'B4', 'C#5', 'G3', '11.4', 'E5', 'C#5', 'D5', '11.2.6', '11.2.6', 'B1', 'B3', 'D4', 'E2', 'E4', 'F#4', 'G4', '7.11', 'E4', 'B-4', 'B4', 'C#5', 'E5', '11.4', 'G3', 'C#5', 'E2', 'B4', 'B-4', 'B3', 'E4', 'G3', 'G4', 'F#4', 'E4', 'F#4', 'E4', 'B3', 'B-4', 'B4', 'E2', 'C#5', '11.4', 'G3', 'E5', 'C#5', 'B4', 'B-4', 'G4', '11.4', 'G3', 'F#4', 'E2', 'E4', 'F#4', '4.7.11', 'G4', 'B-4', 'B4', 'C#5', '11.4', 'G3', 'E5', 'C#5', 'D5', 'B1', 'D5', 'B3', '11.2.6', 'A1', 'F#4', 'A4', 'E4', 'C4', 'A3', '0.4', 'A3', 'A2', '4.9', '9.0', '9.0', 'E4', 'C5', 'F2', 'G#4', '5.8.0', 'G#3', 'C4', 'F4', 'F2', 'G#3', '0.5', 'G#3', 'F4', 'C4', 'F2', 'G#4', 'B4', '5.8.0', 'C5', 'E-5', 'D5', 'C5', 'G#3', 'B4', '0.5', 'G#4', 'F2', 'C5', 'C5', '5.8', 'C4', '5.8', 'C4', 'B4', 'D2', 'D5', 'F4', 'A3', 'D4', 'F4', '9.2', 'D2', 'F4', '9.2', 'A3', 'F2', 'G#4', 'B4', '5.8', 'C5', 'C4', 'E-5', 'D5', 'C5', 'B4', 'G#3', '0.5', 'G#4', 'F2', 'C5', 'C5', '5.8', 'C4', '5.8', 'C4', 'B4', 'D2', 'D5', 'F4', 'D4', 'A3', 'F4', '9.2', 'A2', 'A5', 'A5', 'E-5', '4.9', 'C4', 'E-5', 'F#5', 'F#5', '9.0.4', 'A2', '4.9', 'E5', '0.4', 'E5', 'E-5', 'E-5', '9.0', 'C4', 'E4', 'C5', 'B4', 'E2', 'B4', 'B3', '4.7.11', 'E2', '4.7.11', '4.7.11', 'A2', 'A5', 'A5', 'E-5', 'A4', '0.4', 'E-5', 'F#5', 'F#5', 'A4', 'E4', 'C4', 'A2', 'E5', '4.9', '0.4', 'E5', 'E-5', 'E-5', '0.4', 'C4', 'A4', 'C5', '11.4', 'B4', '4.7.11', 'B4', 'G3', '11.4', 'B4', 'A1', 'E5', 'E5', 'E4', 'A3', 'C4', '0.4', 'A3', 'A2', '4.9', 'C4', '9.0', 'E4'])\n",
      " list(['F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', 'B4', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'E5', 'F5', 'F2', '5.8.0', 'E5', 'F5', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', 'B4', 'F2', '5.8.0', 'G#4', 'G4', '8.0', 'F4', 'G4', 'C3', 'F4', 'G4', '0.3.7', '0.3.7', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', 'B4', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'E5', 'F5', 'F2', '5.8.0', 'E5', 'F5', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', 'B4', 'F2', '5.8.0', 'G#4', 'G4', '8.0', 'F4', 'G4', 'C3', 'F4', 'G4', '0.3.7', '0.3.7', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5', 'B5', '5.8.0', 'G5', 'F5', 'F2', 'G5', 'F5', '5.8.0', '5.8.0', 'G5', 'F5', 'F2', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5', 'C5', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5', 'B5', '5.8.0', 'G5', 'F5', 'F2', 'G5', 'F5', '5.8.0', '5.8.0', 'G5', 'F5', 'F2', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5', 'C5', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'E5', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'C5', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '5.8.0', 'E5', 'F5', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5', 'B5', '5.8.0', 'G5', 'F5', 'F2', 'G5', 'F5', '5.8.0', '5.8.0', 'G5', 'F5', 'F2', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5', 'C5', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5', 'B5', '5.8.0', 'G5', 'F5', 'F2', 'G5', 'F5', '5.8.0', '5.8.0', 'G5', 'F5', 'F2', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5', 'C5', 'F2', '5.8.0', 'C5', 'B4', '5.8.0', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'G4', 'F2', 'G4', 'F4', '8.0', '5.8.0', 'F2', '5.8.0', '5.8.0'])\n",
      " list(['F2', '8.0', 'C5', '5.8', 'E-5', 'D5', 'C5', '0.5', 'G#3', 'C5', 'B4', 'F2', '8.0', 'F4', 'C5', 'B4', '5.8.0', 'F2', '5.8.0', 'C5', 'E-5', 'D5', 'G#3', 'C5', 'C4', 'F4', 'E5', 'F2', 'F5', '5.8.0', 'E5', 'F5', '8.0', 'F4', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', '0.5', 'G#3', 'C5', 'F2', 'B4', '5.8', 'C4', 'G#4', 'G4', 'G#3', '0.5', 'F4', 'C3', 'G4', 'F4', 'G4', 'G3', 'C4', 'F2', 'G#4', 'G4', 'G4', '8.0', 'F4', 'F4', 'G#3', 'C4', 'F2', '5.8', 'C4', '8.0', 'F4', 'F2', '5.8.0', 'C5', 'E-5', 'D5', 'C5', '8.0', 'F4', 'C5', 'B4', 'F2', '5.8.0', 'C5', 'B4', '5.8', '0.5', 'F2', '5.8.0', 'C5', 'E-5', 'D5', 'C5', 'C4', 'F4', 'G#3', 'E5', 'F2', 'F5', '5.8.0', 'E5', 'F5', '8.0', 'F4', 'F2', '8.0', 'C5', 'F4', 'E-5', 'D5', 'C5', 'G#3', '0.5', 'C5', 'F2', 'B4', '5.8.0', 'G#4', 'G4', '5.8', '0.5', 'F4', 'C3', 'G4', 'F4', 'G4', 'C4', 'G3', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', 'F4', 'G#3', 'C4', 'F2', '8.0', 'F4', '5.8', 'C4', 'G#4', 'B-1', 'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'B-4', 'B-1', '10.1.5', 'G#4', 'G4', 'C#4', 'B-3', 'F3', 'G#4', 'F2', 'G4', 'G4', 'F4', '8.0', 'G#3', 'C4', 'F4', 'F2', 'F4', '8.0', 'F4', 'G#3', 'C4', 'G#4', 'B-1', 'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', 'B-1', 'G#4', 'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'G4', 'F2', 'G4', '5.8.0', 'F4', 'G#3', 'C4', 'F2', 'F4', '8.0', 'G#3', '0.5', 'F2', 'C5', 'D5', '5.8.0', 'E5', 'F5', 'G5', 'B5', '0.5', 'G#3', 'G5', 'F5', 'F2', 'G5', '5.8.0', 'F5', '0.5', 'G#3', 'G5', 'F2', 'F5', '5.8.0', 'F5', 'E5', '0.5', 'G#3', 'C#5', 'F2', 'C5', '5.8.0', 'C5', 'B4', 'G#3', '0.5', 'G#4', 'F2', 'G4', 'G4', 'F4', 'G#3', 'C4', 'F4', 'G#3', 'C4', 'F2', 'F4', 'G#3', 'C4', 'F4', 'G#3', 'C4', 'F2', 'C5', '5.8.0', 'D5', 'E5', 'F5', 'G5', 'G#3', 'B5', '0.5', 'G5', 'F5', 'F2', 'G5', '5.8', 'C4', 'F5', '5.8', '0.5', 'F2', 'G5', 'F5', '5.8', 'C4', 'F5', 'E5', '5.8', '0.5', 'C#5', 'F2', 'C5', 'G#3', '0.5', 'C5', 'B4', '5.8.0', 'G#4', 'F2', 'G4', 'G4', 'C4', 'G#3', 'F4', 'G#3', 'C4', 'F4', 'F2', '8.0', 'F4', 'G#3', 'F4', 'C4', 'B-1', 'G#4', 'B-4', '10.1.5', 'G#4', 'G4', '10.1.5', 'G#4', 'B-1', 'B-4', '10.1.5', 'G#4', 'G4', 'B-3', 'C#4', 'F3', 'F2', 'G#4', 'G4', 'G4', 'C4', 'G#3', 'F4', '5.8.0', 'F2', '5.8.0', '8.0', '0.5', 'G#4', 'B-1', 'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'B-1', 'G#4', 'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'F2', 'G4', 'G4', '5.8.0', '5.8.0', 'F2', 'G#3', 'F4', 'C4', 'G#3', 'C4', 'F4', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', 'F4', 'G#3', 'C4', 'C5', 'F2', 'B4', '5.8.0', 'G#3', '0.5', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'C5', 'F4', 'G#3', 'C4', 'E5', 'F2', 'F5', '5.8.0', '8.0', 'F4', 'F2', '8.0', 'C5', 'F4', 'E-5', 'D5', '5.8', 'C5', 'C4', 'C5', 'F2', 'B4', '5.8.0', '5.8.0', 'F2', 'C5', '5.8.0', 'E-5', 'D5', 'G#3', 'C5', '0.5', 'E5', 'F2', 'F5', 'G#3', 'F4', 'C4', '5.8', 'C4', 'B-1', 'G#4', 'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'B-1', 'G#4', 'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', '5.8', 'C4', 'F2', '5.8.0', '5.8', 'C4', 'B-1', 'G#4', 'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'B-1', 'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', 'F4', '8.0', 'F2', 'G#3', 'F4', 'C4', 'G#3', 'F4', 'C4', 'F2', 'C5', 'D5', '5.8', 'C4', 'E5', 'F5', 'G5', 'G#3', '0.5', 'B5', 'G5', 'F5', 'F2', 'G5', '5.8.0', 'F5', '5.8', 'C4', 'F2', 'G5', 'F5', '5.8.0', 'F5', 'E5', '5.8.0', 'C#5', 'F2', 'C5', '5.8.0', 'C5', 'B4', 'G#3', '0.5', 'G#4', 'F2', 'G4', '8.0', 'G4', 'F4', 'F4', 'G#3', 'C4', 'F2', '0.5', 'G#3', 'F4', 'G#3', 'C4', 'F2', 'C5', '5.8', 'D5', 'C4', 'E5', 'F5', 'G5', 'G#3', '0.5', 'B5', 'G5', 'F2', 'F5', '5.8.0', 'G5', 'F5', '5.8.0', 'F2', 'G5', 'F5', '5.8', 'C4', 'F5', 'E5', '5.8.0', 'C#5', 'F2', 'C5', '5.8.0', 'C5', 'B4', 'G#3', '0.5', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', 'G#3', 'F4', 'C4', 'F2', '5.8.0', 'G#3', 'F4', 'C4', 'B-1', 'G#4', 'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'B-1', 'B-4', '10.1.5', 'G#4', 'G4', 'C#4', 'B-3', 'F3', 'G#4', 'F2', 'G4', 'G4', '8.0', 'F4', 'F4', '8.0', 'F2', '5.8.0', 'F4', 'G#3', 'C4', 'B-1', 'G#4', 'B-4', '10.1.5', 'G#4', 'G4', '10.1', 'F3', 'B-4', '10.1', 'F3', 'G#4', 'G4', '10.1', 'F3', 'G#4', 'F2', 'G4', 'G4', 'G#3', 'C4', 'F4'])\n",
      " list(['F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', 'C5', 'B4', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2', '0.5', 'G#3', 'E5', 'F5', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', 'G#4', 'G4', 'F4', 'C4', 'G#3', 'G4', 'C3', 'F4', 'G4', '0.3.7', '0.3.7', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', 'C5', 'B4', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2', '0.5', 'G#3', 'E5', 'F5', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', 'G#4', 'G4', 'F4', 'C4', 'G#3', 'G4', 'C3', 'F4', 'G4', '0.3.7', '0.3.7', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', 'C5', '0.5', 'D5', 'G#3', 'E5', 'F5', 'G5', '0.5', 'B5', 'G#3', 'G5', 'F5', 'F2', 'G5', '0.5', 'F5', 'G#3', '0.5', 'G#3', 'G5', 'F5', 'F2', '0.5', 'G#3', 'E5', '0.5', 'G#3', 'C#5', 'C5', 'F2', '0.5', 'G#3', 'B4', '0.5', 'G#3', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', 'C5', '0.5', 'D5', 'G#3', 'E5', 'F5', 'G5', '0.5', 'B5', 'G#3', 'G5', 'F5', 'F2', 'G5', '0.5', 'F5', 'G#3', '0.5', 'G#3', 'G5', 'F5', 'F2', '0.5', 'G#3', 'E5', '0.5', 'G#3', 'C#5', 'C5', 'F2', '0.5', 'G#3', 'B4', '0.5', 'G#3', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'C5', 'G#3', 'B4', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', '0.5', 'C5', 'G#3', 'E-5', 'D5', '0.5', 'G#3', 'E5', 'F5', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', 'C5', '0.5', 'D5', 'G#3', 'E5', 'F5', 'G5', '0.5', 'B5', 'G#3', 'G5', 'F5', 'F2', 'G5', '0.5', 'F5', 'G#3', '0.5', 'G#3', 'G5', 'F5', 'F2', '0.5', 'G#3', 'E5', '0.5', 'G#3', 'C#5', 'C5', 'F2', '0.5', 'G#3', 'B4', '0.5', 'G#3', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'F2', 'C5', '0.5', 'D5', 'G#3', 'E5', 'F5', 'G5', '0.5', 'B5', 'G#3', 'G5', 'F5', 'F2', 'G5', '0.5', 'F5', 'G#3', '0.5', 'G#3', 'G5', 'F5', 'F2', '0.5', 'G#3', 'E5', '0.5', 'G#3', 'C#5', 'C5', 'F2', '0.5', 'G#3', 'B4', '0.5', 'G#3', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3', 'C4', 'G#3', 'F2', '0.5', 'G#3', '0.5', 'G#3', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'B-4', 'B-1', 'C#4', '5.10', 'G#4', 'G4', 'C#4', '5.10', 'G#4', 'G4', 'F2', '0.5', 'G#3'])\n",
      " list(['G2', '11.2.6', 'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', 'F#5', '11.2.6', 'A5', 'G5', 'D2', 'F#5', '6.9.1', 'C#5', 'B4', 'G2', 'C#5', '11.2.6', 'D5', 'A4', 'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', 'F#5', '11.2.6', 'A5', 'G5', 'D2', 'F#5', '6.9.1', 'C#5', 'B4', 'G2', 'C#5', '11.2.6', 'D5', '9.2', '6.9.1', '6.9.1', '6.11', '11.2.6', 'E4', 'E2', '7.11', 'E2', 'D2', '2.5.9', 'A4', 'A1', 'B4', '9.0.4', 'C5', 'E5', 'D2', 'D5', '4.7.11', 'B4', 'D5', 'D2', 'C5', '11.2.4.7', 'B4', 'D5', 'D2', 'D2', 'D5', 'E5', 'D2', 'F5', '5.9.0', 'G5', 'A5', 'D2', 'C5', '9.0.4', 'D5', 'E5', 'D2', 'D5', '11.2.4.7', 'B4', 'D5', 'D2', 'D2', 'D5', '4.7.11', '6.9.1', 'B4', 'B1', 'A4', '11.2.6', 'B4', 'C#5', 'E2', 'D5', 'E5', 'C#5', 'E2', 'D5', '6.9.1', 'E5', 'E2', '11.2.4.7', 'C5', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', 'F#5', '11.2.6', 'A5', 'G5', 'D2', 'F#5', '6.9.1', 'C#5', 'B4', 'G2', 'C#5', '11.2.6', 'D5', 'A4', 'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', '11.2.6', 'D2', '6.9.1', 'G2', 'F#5', '11.2.6', 'A5', 'G5', 'D2', 'F#5', '6.9.1', 'C#5', 'B4', 'G2', 'C#5', '11.2.6', 'D5', '9.2', '6.9.1', '6.9.1', '6.11', '11.2.6', 'E4', 'E2', '7.11', 'E2', 'D2', '2.5.9', 'A4', 'A1', 'B4', '9.0.4', 'C5', 'E5', 'D2', 'D5', '4.7.11', 'B4', 'D5', 'D2', 'C5', '11.2.4.7', 'B4', 'D5', 'D2', 'D2', 'D5', 'E5', 'D2', 'F5', '5.9.0', 'G5', 'A5', 'D2', 'C5', '9.0.4', 'D5', 'E5', 'D2', 'D5', '11.2.4.7', 'B4', 'D5', 'D2', 'D2', 'D5', 'G5', 'E2', '4.7.11', 'F5', 'E2', '2.5.9', 'B4', 'E2', 'C5', '5.9.0', 'F5', 'E5', 'E2', 'D5', '9.0.4', 'C5', 'E5', 'E2', 'D5', '5.9.0', 'C5', 'E2', '11.2.4.7', 'C5', '2.5.9'])]\n"
     ]
    }
   ],
   "source": [
    "print(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output notes sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving every unique note a unique int\n",
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
    "\n",
    "unique_y = list(set(y.ravel()))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B3': 0, 'C#5': 1, 'B-1': 2, 'F2': 3, '10.1.5': 4, '0.5': 5, 'C5': 6, 'G4': 7, 'E4': 8, '4.9': 9, 'D4': 10, '11.4': 11, 'B-4': 12, 'F#4': 13, '5.9.0': 14, 'F3': 15, 'F4': 16, '4.7.11': 17, '10.1': 18, 'D2': 19, 'E-5': 20, '11.2.6': 21, 'A2': 22, 'B5': 23, '11.2.4.7': 24, 'G3': 25, 'G5': 26, 'E2': 27, 'B4': 28, 'E5': 29, '9.0.4': 30, '6.11': 31, 'B-3': 32, '5.8.0': 33, 'A1': 34, 'G#3': 35, 'F#5': 36, '8.0': 37, '0.4': 38, 'F5': 39, '0.3.7': 40, '2.5.9': 41, '5.8': 42, 'G#4': 43, 'A5': 44, 'A4': 45, 'C4': 46, '6.9.1': 47, 'B1': 48, '5.10': 49, '9.0': 50, '7.11': 51, '9.2': 52, 'C3': 53, 'A3': 54, 'G2': 55, 'C#4': 56, 'D5': 57}\n"
     ]
    }
   ],
   "source": [
    "print(x_note_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the input sequence\n",
    "x_sequence=[]\n",
    "for i in x:\n",
    "    lx=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        lx.append(x_note_to_int[j])\n",
    "    x_sequence.append(lx)\n",
    "    \n",
    "x_sequence = np.array(x_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the output sequence\n",
    "y_sequence = np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_sequence, y_sequence, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           5800      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 58)                14906     \n",
      "=================================================================\n",
      "Total params: 229,026\n",
      "Trainable params: 229,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINE WAVENET MODEL\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 1.0654\n",
      "Epoch 00001: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1.0714 - val_loss: 2.2511\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0358\n",
      "Epoch 00002: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 1.0358 - val_loss: 2.2629\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0008\n",
      "Epoch 00003: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 1.0008 - val_loss: 2.2318\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9717\n",
      "Epoch 00004: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.9717 - val_loss: 2.2689\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9517\n",
      "Epoch 00005: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.9517 - val_loss: 2.2582\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9601\n",
      "Epoch 00006: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.9601 - val_loss: 2.2763\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9524\n",
      "Epoch 00007: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.9524 - val_loss: 2.2934\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9007\n",
      "Epoch 00008: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.9007 - val_loss: 2.2931\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8949\n",
      "Epoch 00009: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.8949 - val_loss: 2.3275\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8939\n",
      "Epoch 00010: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8939 - val_loss: 2.3014\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8431\n",
      "Epoch 00011: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8431 - val_loss: 2.3562\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8636\n",
      "Epoch 00012: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8636 - val_loss: 2.3986\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8459\n",
      "Epoch 00013: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8459 - val_loss: 2.3604\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8244\n",
      "Epoch 00014: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8244 - val_loss: 2.3749\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8040\n",
      "Epoch 00015: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.8040 - val_loss: 2.4110\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7973\n",
      "Epoch 00016: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.7973 - val_loss: 2.3806\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7747\n",
      "Epoch 00017: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.7747 - val_loss: 2.4201\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7342\n",
      "Epoch 00018: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.7342 - val_loss: 2.4294\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.7462\n",
      "Epoch 00019: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.7462 - val_loss: 2.4544\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6905\n",
      "Epoch 00020: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.6905 - val_loss: 2.4921\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6939\n",
      "Epoch 00021: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.6939 - val_loss: 2.5059\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6502\n",
      "Epoch 00022: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.6502 - val_loss: 2.5239\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6367\n",
      "Epoch 00023: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6367 - val_loss: 2.5605\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6438\n",
      "Epoch 00024: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6438 - val_loss: 2.5424\n",
      "Epoch 25/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.6465\n",
      "Epoch 00025: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.6468 - val_loss: 2.5254\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6050\n",
      "Epoch 00026: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.6050 - val_loss: 2.5856\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6111\n",
      "Epoch 00027: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.6111 - val_loss: 2.5124\n",
      "Epoch 28/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.5934\n",
      "Epoch 00028: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5851 - val_loss: 2.5566\n",
      "Epoch 29/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.6030\n",
      "Epoch 00029: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5998 - val_loss: 2.6292\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5978\n",
      "Epoch 00030: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5978 - val_loss: 2.6585\n",
      "Epoch 31/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.5541\n",
      "Epoch 00031: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5533 - val_loss: 2.7196\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5327\n",
      "Epoch 00032: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5327 - val_loss: 2.6286\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5248\n",
      "Epoch 00033: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.5248 - val_loss: 2.7226\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5251\n",
      "Epoch 00034: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5251 - val_loss: 2.8322\n",
      "Epoch 35/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.5538\n",
      "Epoch 00035: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5585 - val_loss: 2.7365\n",
      "Epoch 36/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.5029\n",
      "Epoch 00036: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.5103 - val_loss: 2.6507\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5280\n",
      "Epoch 00037: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5280 - val_loss: 2.8219\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5379\n",
      "Epoch 00038: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.5379 - val_loss: 2.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5099\n",
      "Epoch 00039: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5099 - val_loss: 2.7028\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4708\n",
      "Epoch 00040: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4708 - val_loss: 2.7756\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4590\n",
      "Epoch 00041: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4590 - val_loss: 2.7957\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4574\n",
      "Epoch 00042: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4574 - val_loss: 2.8049\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4315\n",
      "Epoch 00043: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4315 - val_loss: 2.8080\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4270\n",
      "Epoch 00044: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4270 - val_loss: 2.8834\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4491\n",
      "Epoch 00045: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4491 - val_loss: 2.9237\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3943\n",
      "Epoch 00046: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3943 - val_loss: 2.8768\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4243\n",
      "Epoch 00047: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4243 - val_loss: 2.9416\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4050\n",
      "Epoch 00048: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4050 - val_loss: 2.9051\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4053\n",
      "Epoch 00049: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4053 - val_loss: 2.9192\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4346\n",
      "Epoch 00050: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4346 - val_loss: 2.8610\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4017\n",
      "Epoch 00051: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4017 - val_loss: 2.8548\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3984\n",
      "Epoch 00052: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3984 - val_loss: 2.9719\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3660\n",
      "Epoch 00053: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3660 - val_loss: 3.0423\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3797\n",
      "Epoch 00054: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3797 - val_loss: 3.0188\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4128\n",
      "Epoch 00055: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.4128 - val_loss: 2.9223\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3703\n",
      "Epoch 00056: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3703 - val_loss: 2.9093\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3968\n",
      "Epoch 00057: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3968 - val_loss: 2.9312\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3543\n",
      "Epoch 00058: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3543 - val_loss: 2.9704\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3734\n",
      "Epoch 00059: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3734 - val_loss: 2.9850\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3370\n",
      "Epoch 00060: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3370 - val_loss: 3.0581\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3506\n",
      "Epoch 00061: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3506 - val_loss: 3.0572\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3211\n",
      "Epoch 00062: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3211 - val_loss: 3.0139\n",
      "Epoch 63/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.3116\n",
      "Epoch 00063: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3267 - val_loss: 3.0602\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3611\n",
      "Epoch 00064: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3611 - val_loss: 3.0886\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3304\n",
      "Epoch 00065: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3304 - val_loss: 3.1068\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3405\n",
      "Epoch 00066: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3405 - val_loss: 3.0486\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3231\n",
      "Epoch 00067: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3231 - val_loss: 3.0665\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3199\n",
      "Epoch 00068: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3199 - val_loss: 3.0806\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3040\n",
      "Epoch 00069: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.3040 - val_loss: 3.0782\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3210\n",
      "Epoch 00070: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3210 - val_loss: 3.1199\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2896\n",
      "Epoch 00071: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2896 - val_loss: 3.1236\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3187\n",
      "Epoch 00072: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3187 - val_loss: 3.1435\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2962\n",
      "Epoch 00073: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2962 - val_loss: 3.0894\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3068\n",
      "Epoch 00074: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3068 - val_loss: 3.0446\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2748\n",
      "Epoch 00075: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2748 - val_loss: 3.1162\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2641\n",
      "Epoch 00076: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2641 - val_loss: 3.2438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2856\n",
      "Epoch 00077: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2856 - val_loss: 3.3215\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2433\n",
      "Epoch 00078: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2433 - val_loss: 3.2123\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2800\n",
      "Epoch 00079: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2800 - val_loss: 3.2678\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2899\n",
      "Epoch 00080: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2899 - val_loss: 3.2169\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2904\n",
      "Epoch 00081: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2904 - val_loss: 3.1661\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3134\n",
      "Epoch 00082: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3134 - val_loss: 3.1686\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2668\n",
      "Epoch 00083: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2668 - val_loss: 3.2283\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2769\n",
      "Epoch 00084: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2769 - val_loss: 3.2174\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2509\n",
      "Epoch 00085: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2509 - val_loss: 3.2981\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2648\n",
      "Epoch 00086: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2648 - val_loss: 3.3424\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2695\n",
      "Epoch 00087: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2695 - val_loss: 3.2618\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2745\n",
      "Epoch 00088: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2745 - val_loss: 3.2897\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2474\n",
      "Epoch 00089: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2474 - val_loss: 3.2623\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2508\n",
      "Epoch 00090: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2508 - val_loss: 3.3562\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2353\n",
      "Epoch 00091: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2353 - val_loss: 3.3601\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2788\n",
      "Epoch 00092: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2788 - val_loss: 3.3829\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2908\n",
      "Epoch 00093: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2908 - val_loss: 3.3739\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2305\n",
      "Epoch 00094: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2305 - val_loss: 3.2405\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2183\n",
      "Epoch 00095: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2183 - val_loss: 3.3063\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2400\n",
      "Epoch 00096: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2400 - val_loss: 3.3750\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2339\n",
      "Epoch 00097: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.2339 - val_loss: 3.3730\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2313\n",
      "Epoch 00098: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2313 - val_loss: 3.3927\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2298\n",
      "Epoch 00099: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2298 - val_loss: 3.4543\n",
      "Epoch 100/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.2438\n",
      "Epoch 00100: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.2428 - val_loss: 3.4217\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2219\n",
      "Epoch 00101: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2219 - val_loss: 3.3670\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2460\n",
      "Epoch 00102: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2460 - val_loss: 3.4372\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2153\n",
      "Epoch 00103: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2153 - val_loss: 3.4069\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2371\n",
      "Epoch 00104: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2371 - val_loss: 3.3927\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2273\n",
      "Epoch 00105: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2273 - val_loss: 3.4420\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2247\n",
      "Epoch 00106: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2247 - val_loss: 3.4672\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2219\n",
      "Epoch 00107: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2219 - val_loss: 3.4662\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2084\n",
      "Epoch 00108: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2084 - val_loss: 3.3677\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2202\n",
      "Epoch 00109: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2202 - val_loss: 3.3825\n",
      "Epoch 110/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.2465\n",
      "Epoch 00110: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2453 - val_loss: 3.4018\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2357\n",
      "Epoch 00111: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2357 - val_loss: 3.4116\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1770\n",
      "Epoch 00112: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1770 - val_loss: 3.4092\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00113: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1844 - val_loss: 3.5299\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00114: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2025 - val_loss: 3.5655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2013\n",
      "Epoch 00115: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2013 - val_loss: 3.5411\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00116: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2063 - val_loss: 3.4940\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2238\n",
      "Epoch 00117: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2238 - val_loss: 3.5764\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1788\n",
      "Epoch 00118: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1788 - val_loss: 3.6371\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2144\n",
      "Epoch 00119: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2144 - val_loss: 3.5440\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1927\n",
      "Epoch 00120: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1927 - val_loss: 3.5348\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1976\n",
      "Epoch 00121: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1976 - val_loss: 3.5840\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1803\n",
      "Epoch 00122: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1803 - val_loss: 3.4524\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1965\n",
      "Epoch 00123: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1965 - val_loss: 3.6036\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00124: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1877 - val_loss: 3.5510\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1775\n",
      "Epoch 00125: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1775 - val_loss: 3.5755\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1981\n",
      "Epoch 00126: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1981 - val_loss: 3.6397\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1822\n",
      "Epoch 00127: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1822 - val_loss: 3.5570\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1806\n",
      "Epoch 00128: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1806 - val_loss: 3.5391\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1732\n",
      "Epoch 00129: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1732 - val_loss: 3.5743\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1740\n",
      "Epoch 00130: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1740 - val_loss: 3.6106\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1678\n",
      "Epoch 00131: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1678 - val_loss: 3.6722\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1768\n",
      "Epoch 00132: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1768 - val_loss: 3.7014\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1747\n",
      "Epoch 00133: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1747 - val_loss: 3.7165\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2000\n",
      "Epoch 00134: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2000 - val_loss: 3.6610\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1848\n",
      "Epoch 00135: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1848 - val_loss: 3.6726\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1732\n",
      "Epoch 00136: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1732 - val_loss: 3.6967\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00137: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1670 - val_loss: 3.8155\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1908\n",
      "Epoch 00138: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1908 - val_loss: 3.6953\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1689\n",
      "Epoch 00139: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1689 - val_loss: 3.7318\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1762\n",
      "Epoch 00140: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1762 - val_loss: 3.7512\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1706\n",
      "Epoch 00141: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1706 - val_loss: 3.7440\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1744\n",
      "Epoch 00142: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1744 - val_loss: 3.7107\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2004\n",
      "Epoch 00143: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2004 - val_loss: 3.6726\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1646\n",
      "Epoch 00144: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1646 - val_loss: 3.6107\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1606\n",
      "Epoch 00145: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1606 - val_loss: 3.7648\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00146: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1892 - val_loss: 3.6722\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1749\n",
      "Epoch 00147: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1749 - val_loss: 3.6840\n",
      "Epoch 148/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1724\n",
      "Epoch 00148: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1820 - val_loss: 3.6445\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1541\n",
      "Epoch 00149: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1541 - val_loss: 3.6921\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1693\n",
      "Epoch 00150: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1693 - val_loss: 3.6873\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1560\n",
      "Epoch 00151: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1560 - val_loss: 3.8347\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1480\n",
      "Epoch 00152: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1480 - val_loss: 3.8320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1809\n",
      "Epoch 00153: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1809 - val_loss: 3.8039\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1687\n",
      "Epoch 00154: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1687 - val_loss: 3.8585\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1568\n",
      "Epoch 00155: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1568 - val_loss: 3.8444\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1594\n",
      "Epoch 00156: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1594 - val_loss: 3.9632\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1573\n",
      "Epoch 00157: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1573 - val_loss: 3.9013\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00158: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1242 - val_loss: 3.8682\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00159: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1279 - val_loss: 3.9456\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1548\n",
      "Epoch 00160: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1548 - val_loss: 3.9859\n",
      "Epoch 161/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00161: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1512 - val_loss: 3.8428\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1649\n",
      "Epoch 00162: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1649 - val_loss: 3.8898\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1507\n",
      "Epoch 00163: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1507 - val_loss: 3.8321\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1921\n",
      "Epoch 00164: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1921 - val_loss: 3.8576\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1652\n",
      "Epoch 00165: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1652 - val_loss: 3.8542\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1468\n",
      "Epoch 00166: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1468 - val_loss: 3.9252\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1586\n",
      "Epoch 00167: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1586 - val_loss: 3.8979\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1355\n",
      "Epoch 00168: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1355 - val_loss: 3.9593\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1528\n",
      "Epoch 00169: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1528 - val_loss: 3.9374\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1368\n",
      "Epoch 00170: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1368 - val_loss: 3.8398\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00171: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1324 - val_loss: 3.9432\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1646\n",
      "Epoch 00172: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1646 - val_loss: 3.9604\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1703\n",
      "Epoch 00173: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1703 - val_loss: 3.9259\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1383\n",
      "Epoch 00174: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1383 - val_loss: 3.9572\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1563\n",
      "Epoch 00175: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1563 - val_loss: 3.9087\n",
      "Epoch 176/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1325\n",
      "Epoch 00176: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1336 - val_loss: 3.9449\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1495\n",
      "Epoch 00177: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1495 - val_loss: 3.8797\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1517\n",
      "Epoch 00178: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1517 - val_loss: 3.9536\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00179: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1475 - val_loss: 3.9480\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1488\n",
      "Epoch 00180: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1488 - val_loss: 3.9896\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1593\n",
      "Epoch 00181: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1593 - val_loss: 3.9394\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1721\n",
      "Epoch 00182: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1721 - val_loss: 3.9650\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1465\n",
      "Epoch 00183: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1465 - val_loss: 3.9200\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00184: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1153 - val_loss: 3.9969\n",
      "Epoch 185/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1376\n",
      "Epoch 00185: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1386 - val_loss: 3.9807\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1312\n",
      "Epoch 00186: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1312 - val_loss: 4.0374\n",
      "Epoch 187/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1219\n",
      "Epoch 00187: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1237 - val_loss: 3.9996\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1207\n",
      "Epoch 00188: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1207 - val_loss: 4.0277\n",
      "Epoch 189/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1387\n",
      "Epoch 00189: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1412 - val_loss: 4.0173\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1549\n",
      "Epoch 00190: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1549 - val_loss: 4.0173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1560\n",
      "Epoch 00191: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1560 - val_loss: 4.0175\n",
      "Epoch 192/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1412\n",
      "Epoch 00192: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1402 - val_loss: 3.9369\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1707\n",
      "Epoch 00193: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1707 - val_loss: 3.9225\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1517\n",
      "Epoch 00194: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1517 - val_loss: 3.8939\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00195: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1236 - val_loss: 4.0286\n",
      "Epoch 196/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1193\n",
      "Epoch 00196: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1205 - val_loss: 3.9455\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1196\n",
      "Epoch 00197: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1196 - val_loss: 4.0186\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1347\n",
      "Epoch 00198: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.1347 - val_loss: 3.9660\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1380\n",
      "Epoch 00199: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1380 - val_loss: 3.9962\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00200: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1234 - val_loss: 4.1616\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00201: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1305 - val_loss: 4.0638\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1387\n",
      "Epoch 00202: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1387 - val_loss: 4.1601\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1379\n",
      "Epoch 00203: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1379 - val_loss: 4.1583\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1306\n",
      "Epoch 00204: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1306 - val_loss: 4.1359\n",
      "Epoch 205/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1347\n",
      "Epoch 00205: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1347 - val_loss: 4.1397\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1351\n",
      "Epoch 00206: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1351 - val_loss: 4.0860\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00207: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1527 - val_loss: 4.0106\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1514\n",
      "Epoch 00208: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1514 - val_loss: 4.0709\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1433\n",
      "Epoch 00209: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1433 - val_loss: 4.0311\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00210: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1213 - val_loss: 3.9150\n",
      "Epoch 211/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00211: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1276 - val_loss: 4.1210\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00212: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1152 - val_loss: 4.0855\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 00213: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0985 - val_loss: 4.1559\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00214: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1299 - val_loss: 4.0833\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1364\n",
      "Epoch 00215: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1364 - val_loss: 4.1144\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1351\n",
      "Epoch 00216: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1351 - val_loss: 4.0550\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 00217: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1069 - val_loss: 4.1486\n",
      "Epoch 218/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1427\n",
      "Epoch 00218: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1362 - val_loss: 4.2335\n",
      "Epoch 219/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00219: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1123 - val_loss: 4.1680\n",
      "Epoch 220/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00220: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1191 - val_loss: 4.2295\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00221: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1149 - val_loss: 4.1051\n",
      "Epoch 222/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.1360\n",
      "Epoch 00222: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1435 - val_loss: 4.3001\n",
      "Epoch 223/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1216\n",
      "Epoch 00223: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1249 - val_loss: 4.1547\n",
      "Epoch 224/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1431\n",
      "Epoch 00224: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1413 - val_loss: 4.1246\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00225: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1183 - val_loss: 4.2129\n",
      "Epoch 226/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1123\n",
      "Epoch 00226: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1133 - val_loss: 4.2183\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1321\n",
      "Epoch 00227: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.1321 - val_loss: 4.1957\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00228: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1021 - val_loss: 4.2210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00229: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0975 - val_loss: 4.1946\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00230: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0991 - val_loss: 4.2889\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00231: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1289 - val_loss: 4.2460\n",
      "Epoch 232/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00232: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1174 - val_loss: 4.2546\n",
      "Epoch 233/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1164\n",
      "Epoch 00233: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1145 - val_loss: 4.2463\n",
      "Epoch 234/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00234: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1312 - val_loss: 4.2665\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 00235: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1065 - val_loss: 4.1823\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00236: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1088 - val_loss: 4.2363\n",
      "Epoch 237/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00237: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1046 - val_loss: 4.3280\n",
      "Epoch 238/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0962\n",
      "Epoch 00238: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1045 - val_loss: 4.3886\n",
      "Epoch 239/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00239: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1056 - val_loss: 4.2889\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0956\n",
      "Epoch 00240: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0956 - val_loss: 4.3514\n",
      "Epoch 241/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1055\n",
      "Epoch 00241: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1070 - val_loss: 4.3188\n",
      "Epoch 242/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1212\n",
      "Epoch 00242: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1167 - val_loss: 4.2769\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00243: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1225 - val_loss: 4.2324\n",
      "Epoch 244/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00244: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0995 - val_loss: 4.3298\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00245: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1106 - val_loss: 4.3300\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00246: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1229 - val_loss: 4.3955\n",
      "Epoch 247/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00247: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1134 - val_loss: 4.3608\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00248: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1244 - val_loss: 4.3129\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1215\n",
      "Epoch 00249: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1215 - val_loss: 4.3456\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1378\n",
      "Epoch 00250: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1378 - val_loss: 4.2394\n",
      "Epoch 251/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1066\n",
      "Epoch 00251: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1078 - val_loss: 4.2790\n",
      "Epoch 252/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00252: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1106 - val_loss: 4.3024\n",
      "Epoch 253/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00253: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1169 - val_loss: 4.3783\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00254: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1256 - val_loss: 4.3098\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 00255: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1048 - val_loss: 4.2673\n",
      "Epoch 256/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1174\n",
      "Epoch 00256: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1201 - val_loss: 4.3743\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 00257: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1050 - val_loss: 4.2259\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00258: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1262 - val_loss: 4.2715\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 00259: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1059 - val_loss: 4.2528\n",
      "Epoch 260/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00260: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1151 - val_loss: 4.4519\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 00261: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1026 - val_loss: 4.4271\n",
      "Epoch 262/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0952\n",
      "Epoch 00262: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0978 - val_loss: 4.3702\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00263: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1095 - val_loss: 4.3343\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 00264: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1000 - val_loss: 4.4337\n",
      "Epoch 265/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00265: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1212 - val_loss: 4.4098\n",
      "Epoch 266/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00266: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1218 - val_loss: 4.2736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 00267: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1061 - val_loss: 4.3099\n",
      "Epoch 268/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00268: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1216 - val_loss: 4.2665\n",
      "Epoch 269/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1071\n",
      "Epoch 00269: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1075 - val_loss: 4.1745\n",
      "Epoch 270/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00270: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0962 - val_loss: 4.3119\n",
      "Epoch 271/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1028\n",
      "Epoch 00271: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1021 - val_loss: 4.3503\n",
      "Epoch 272/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0938\n",
      "Epoch 00272: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0959 - val_loss: 4.3544\n",
      "Epoch 273/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0945\n",
      "Epoch 00273: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0934 - val_loss: 4.4236\n",
      "Epoch 274/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00274: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.1019 - val_loss: 4.4581\n",
      "Epoch 275/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1064\n",
      "Epoch 00275: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1084 - val_loss: 4.3845\n",
      "Epoch 276/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1009\n",
      "Epoch 00276: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1005 - val_loss: 4.4044\n",
      "Epoch 277/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0954\n",
      "Epoch 00277: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0981 - val_loss: 4.3806\n",
      "Epoch 278/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1075\n",
      "Epoch 00278: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1088 - val_loss: 4.3883\n",
      "Epoch 279/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0931\n",
      "Epoch 00279: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0930 - val_loss: 4.3387\n",
      "Epoch 280/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00280: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0973 - val_loss: 4.3793\n",
      "Epoch 281/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1056\n",
      "Epoch 00281: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1053 - val_loss: 4.2985\n",
      "Epoch 282/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0824\n",
      "Epoch 00282: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0850 - val_loss: 4.4473\n",
      "Epoch 283/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0992\n",
      "Epoch 00283: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1017 - val_loss: 4.3915\n",
      "Epoch 284/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00284: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.1201 - val_loss: 4.3350\n",
      "Epoch 285/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00285: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.1087 - val_loss: 4.4306\n",
      "Epoch 286/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00286: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1187 - val_loss: 4.4089\n",
      "Epoch 287/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1023\n",
      "Epoch 00287: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1090 - val_loss: 4.4072\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 00288: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1029 - val_loss: 4.4913\n",
      "Epoch 289/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0895\n",
      "Epoch 00289: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0979 - val_loss: 4.3995\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00290: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1006 - val_loss: 4.3404\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 00291: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0884 - val_loss: 4.4205\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 00292: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1001 - val_loss: 4.4620\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00293: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0823 - val_loss: 4.3708\n",
      "Epoch 294/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00294: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0947 - val_loss: 4.3860\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0914\n",
      "Epoch 00295: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0914 - val_loss: 4.4568\n",
      "Epoch 296/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0969\n",
      "Epoch 00296: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0992 - val_loss: 4.4715\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0906\n",
      "Epoch 00297: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0906 - val_loss: 4.6257\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00298: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0946 - val_loss: 4.6764\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 00299: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1066 - val_loss: 4.4550\n",
      "Epoch 300/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.1177\n",
      "Epoch 00300: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1142 - val_loss: 4.4013\n",
      "Epoch 301/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0878\n",
      "Epoch 00301: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0878 - val_loss: 4.4969\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 00302: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0996 - val_loss: 4.5459\n",
      "Epoch 303/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1050\n",
      "Epoch 00303: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1046 - val_loss: 4.5357\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00304: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1006 - val_loss: 4.5076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0872\n",
      "Epoch 00305: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0889 - val_loss: 4.5135\n",
      "Epoch 306/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0958\n",
      "Epoch 00306: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0957 - val_loss: 4.6817\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 00307: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0991 - val_loss: 4.6394\n",
      "Epoch 308/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1015\n",
      "Epoch 00308: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1043 - val_loss: 4.5932\n",
      "Epoch 309/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0876\n",
      "Epoch 00309: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0850 - val_loss: 4.5855\n",
      "Epoch 310/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0748\n",
      "Epoch 00310: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0782 - val_loss: 4.6188\n",
      "Epoch 311/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0754\n",
      "Epoch 00311: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0750 - val_loss: 4.6861\n",
      "Epoch 312/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0880\n",
      "Epoch 00312: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0851 - val_loss: 4.7711\n",
      "Epoch 313/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0831\n",
      "Epoch 00313: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0846 - val_loss: 4.7023\n",
      "Epoch 314/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0949\n",
      "Epoch 00314: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 0.0921 - val_loss: 4.7106\n",
      "Epoch 315/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0843\n",
      "Epoch 00315: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0855 - val_loss: 4.6520\n",
      "Epoch 316/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1053\n",
      "Epoch 00316: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.1020 - val_loss: 4.6289\n",
      "Epoch 317/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00317: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.1185 - val_loss: 4.5755\n",
      "Epoch 318/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0972\n",
      "Epoch 00318: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0979 - val_loss: 4.6154\n",
      "Epoch 319/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00319: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.1220 - val_loss: 4.6185\n",
      "Epoch 320/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0973\n",
      "Epoch 00320: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0967 - val_loss: 4.4999\n",
      "Epoch 321/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0878\n",
      "Epoch 00321: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0868 - val_loss: 4.5339\n",
      "Epoch 322/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0916\n",
      "Epoch 00322: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0907 - val_loss: 4.5542\n",
      "Epoch 323/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0891\n",
      "Epoch 00323: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0900 - val_loss: 4.6062\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00324: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1188 - val_loss: 4.3826\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0927\n",
      "Epoch 00325: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0927 - val_loss: 4.5468\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0885\n",
      "Epoch 00326: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0885 - val_loss: 4.4756\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0773\n",
      "Epoch 00327: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0773 - val_loss: 4.5346\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 00328: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0976 - val_loss: 4.5812\n",
      "Epoch 329/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0967\n",
      "Epoch 00329: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0955 - val_loss: 4.6524\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0908\n",
      "Epoch 00330: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0908 - val_loss: 4.7512\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0827\n",
      "Epoch 00331: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0827 - val_loss: 4.6527\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0814\n",
      "Epoch 00332: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.0814 - val_loss: 4.6391\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0774\n",
      "Epoch 00333: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0774 - val_loss: 4.5621\n",
      "Epoch 334/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0876\n",
      "Epoch 00334: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0894 - val_loss: 4.6579\n",
      "Epoch 335/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0946\n",
      "Epoch 00335: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0906 - val_loss: 4.6518\n",
      "Epoch 336/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0892\n",
      "Epoch 00336: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0883 - val_loss: 4.6007\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0841\n",
      "Epoch 00337: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0841 - val_loss: 4.7848\n",
      "Epoch 338/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0951\n",
      "Epoch 00338: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0953 - val_loss: 4.6985\n",
      "Epoch 339/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0883\n",
      "Epoch 00339: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0877 - val_loss: 4.5899\n",
      "Epoch 340/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00340: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0965 - val_loss: 4.6659\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0823\n",
      "Epoch 00341: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0823 - val_loss: 4.7278\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0920\n",
      "Epoch 00342: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0920 - val_loss: 4.6695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.1045\n",
      "Epoch 00343: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0995 - val_loss: 4.5749\n",
      "Epoch 344/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0793\n",
      "Epoch 00344: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0789 - val_loss: 4.7269\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00345: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0978 - val_loss: 4.6492\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00346: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1084 - val_loss: 4.7609\n",
      "Epoch 347/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1058\n",
      "Epoch 00347: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1036 - val_loss: 4.6152\n",
      "Epoch 348/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0994\n",
      "Epoch 00348: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0981 - val_loss: 4.7137\n",
      "Epoch 349/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1212\n",
      "Epoch 00349: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1203 - val_loss: 4.6711\n",
      "Epoch 350/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0878\n",
      "Epoch 00350: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0911 - val_loss: 4.7260\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0884\n",
      "Epoch 00351: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0884 - val_loss: 4.5556\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00352: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1105 - val_loss: 4.6117\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 00353: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0987 - val_loss: 4.6009\n",
      "Epoch 354/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0905\n",
      "Epoch 00354: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0874 - val_loss: 4.7720\n",
      "Epoch 355/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0931\n",
      "Epoch 00355: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0877 - val_loss: 4.6749\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 00356: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1023 - val_loss: 4.6469\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0703\n",
      "Epoch 00357: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0703 - val_loss: 4.7695\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0803\n",
      "Epoch 00358: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0803 - val_loss: 4.7426\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 00359: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1013 - val_loss: 4.8464\n",
      "Epoch 360/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0813\n",
      "Epoch 00360: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0829 - val_loss: 4.7966\n",
      "Epoch 361/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0990\n",
      "Epoch 00361: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0993 - val_loss: 4.8707\n",
      "Epoch 362/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0741\n",
      "Epoch 00362: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0758 - val_loss: 4.6812\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0922\n",
      "Epoch 00363: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0922 - val_loss: 4.6139\n",
      "Epoch 364/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0867\n",
      "Epoch 00364: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0872 - val_loss: 4.7504\n",
      "Epoch 365/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00365: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0961 - val_loss: 4.7207\n",
      "Epoch 366/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0776\n",
      "Epoch 00366: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0845 - val_loss: 4.7083\n",
      "Epoch 367/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0887\n",
      "Epoch 00367: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0995 - val_loss: 4.7021\n",
      "Epoch 368/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0866\n",
      "Epoch 00368: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0849 - val_loss: 4.7484\n",
      "Epoch 369/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0667\n",
      "Epoch 00369: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0666 - val_loss: 4.7473\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0721\n",
      "Epoch 00370: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0721 - val_loss: 4.7822\n",
      "Epoch 371/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0959\n",
      "Epoch 00371: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0967 - val_loss: 4.8118\n",
      "Epoch 372/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0964\n",
      "Epoch 00372: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0938 - val_loss: 4.8686\n",
      "Epoch 373/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0832\n",
      "Epoch 00373: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0826 - val_loss: 4.7858\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0931\n",
      "Epoch 00374: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0931 - val_loss: 4.9268\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 00375: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0966 - val_loss: 4.6988\n",
      "Epoch 376/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0907\n",
      "Epoch 00376: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0954 - val_loss: 4.7314\n",
      "Epoch 377/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0882\n",
      "Epoch 00377: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0877 - val_loss: 4.6291\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0783\n",
      "Epoch 00378: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0783 - val_loss: 4.8113\n",
      "Epoch 379/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0614\n",
      "Epoch 00379: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0694 - val_loss: 4.7717\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0611\n",
      "Epoch 00380: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0611 - val_loss: 4.6894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0955\n",
      "Epoch 00381: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0964 - val_loss: 4.8331\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0937\n",
      "Epoch 00382: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0937 - val_loss: 4.7585\n",
      "Epoch 383/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0815\n",
      "Epoch 00383: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0823 - val_loss: 4.7548\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0805\n",
      "Epoch 00384: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0805 - val_loss: 4.8585\n",
      "Epoch 385/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0833\n",
      "Epoch 00385: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0834 - val_loss: 4.8366\n",
      "Epoch 386/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0839\n",
      "Epoch 00386: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0856 - val_loss: 4.8517\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0849\n",
      "Epoch 00387: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0849 - val_loss: 4.8371\n",
      "Epoch 388/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0877\n",
      "Epoch 00388: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0886 - val_loss: 4.8092\n",
      "Epoch 389/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0762\n",
      "Epoch 00389: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0731 - val_loss: 4.8601\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 00390: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0777 - val_loss: 4.9186\n",
      "Epoch 391/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1024\n",
      "Epoch 00391: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1018 - val_loss: 4.8285\n",
      "Epoch 392/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0913\n",
      "Epoch 00392: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1024 - val_loss: 4.8574\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 00393: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0970 - val_loss: 4.9016\n",
      "Epoch 394/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00394: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1186 - val_loss: 4.7838\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 00395: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.1006 - val_loss: 4.7799\n",
      "Epoch 396/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0884\n",
      "Epoch 00396: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0836 - val_loss: 4.6928\n",
      "Epoch 397/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0714\n",
      "Epoch 00397: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0715 - val_loss: 4.7171\n",
      "Epoch 398/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0729\n",
      "Epoch 00398: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0742 - val_loss: 4.7772\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 00399: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0880 - val_loss: 4.8233\n",
      "Epoch 400/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0908\n",
      "Epoch 00400: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0915 - val_loss: 4.6841\n",
      "Epoch 401/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0901\n",
      "Epoch 00401: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0860 - val_loss: 4.7465\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0810\n",
      "Epoch 00402: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0810 - val_loss: 4.6616\n",
      "Epoch 403/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0747\n",
      "Epoch 00403: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0725 - val_loss: 4.6549\n",
      "Epoch 404/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0744\n",
      "Epoch 00404: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0776 - val_loss: 4.7956\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0895\n",
      "Epoch 00405: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0895 - val_loss: 4.7647\n",
      "Epoch 406/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0794\n",
      "Epoch 00406: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0811 - val_loss: 4.8115\n",
      "Epoch 407/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0707\n",
      "Epoch 00407: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0738 - val_loss: 4.7287\n",
      "Epoch 408/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0822\n",
      "Epoch 00408: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0808 - val_loss: 4.7542\n",
      "Epoch 409/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0629\n",
      "Epoch 00409: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0612 - val_loss: 4.8563\n",
      "Epoch 410/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0759\n",
      "Epoch 00410: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0792 - val_loss: 4.7762\n",
      "Epoch 411/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0684\n",
      "Epoch 00411: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0679 - val_loss: 4.7760\n",
      "Epoch 412/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0784\n",
      "Epoch 00412: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0821 - val_loss: 4.8031\n",
      "Epoch 413/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0861\n",
      "Epoch 00413: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0838 - val_loss: 4.8576\n",
      "Epoch 414/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0588\n",
      "Epoch 00414: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0590 - val_loss: 4.8153\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0875\n",
      "Epoch 00415: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0875 - val_loss: 4.7712\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0890\n",
      "Epoch 00416: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0890 - val_loss: 4.7581\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0710\n",
      "Epoch 00417: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0710 - val_loss: 4.8710\n",
      "Epoch 418/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0633\n",
      "Epoch 00418: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0671 - val_loss: 4.8467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 419/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0764\n",
      "Epoch 00419: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0764 - val_loss: 4.7415\n",
      "Epoch 420/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0737\n",
      "Epoch 00420: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0741 - val_loss: 4.6725\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0687\n",
      "Epoch 00421: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0687 - val_loss: 4.7414\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0695\n",
      "Epoch 00422: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0695 - val_loss: 4.8513\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0836\n",
      "Epoch 00423: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0836 - val_loss: 4.8768\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0843\n",
      "Epoch 00424: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0843 - val_loss: 4.8630\n",
      "Epoch 425/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0681\n",
      "Epoch 00425: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0671 - val_loss: 4.7388\n",
      "Epoch 426/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0743\n",
      "Epoch 00426: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0742 - val_loss: 4.8323\n",
      "Epoch 427/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0821\n",
      "Epoch 00427: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0786 - val_loss: 4.7880\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0818\n",
      "Epoch 00428: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0818 - val_loss: 4.9606\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0905\n",
      "Epoch 00429: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0905 - val_loss: 4.7397\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0790\n",
      "Epoch 00430: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0790 - val_loss: 4.7985\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0779\n",
      "Epoch 00431: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0779 - val_loss: 4.7236\n",
      "Epoch 432/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0915\n",
      "Epoch 00432: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0878 - val_loss: 4.7979\n",
      "Epoch 433/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0800\n",
      "Epoch 00433: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0795 - val_loss: 4.7155\n",
      "Epoch 434/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0745\n",
      "Epoch 00434: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0726 - val_loss: 4.7962\n",
      "Epoch 435/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0553\n",
      "Epoch 00435: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0556 - val_loss: 4.8972\n",
      "Epoch 436/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0796\n",
      "Epoch 00436: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0794 - val_loss: 4.9826\n",
      "Epoch 437/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0750\n",
      "Epoch 00437: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0747 - val_loss: 4.8076\n",
      "Epoch 438/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0697\n",
      "Epoch 00438: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0690 - val_loss: 4.9936\n",
      "Epoch 439/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0687\n",
      "Epoch 00439: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0671 - val_loss: 4.8908\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0853\n",
      "Epoch 00440: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0853 - val_loss: 4.9497\n",
      "Epoch 441/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0906\n",
      "Epoch 00441: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0922 - val_loss: 4.8514\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0686\n",
      "Epoch 00442: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0686 - val_loss: 4.8098\n",
      "Epoch 443/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0761\n",
      "Epoch 00443: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0749 - val_loss: 4.9473\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0948\n",
      "Epoch 00444: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0948 - val_loss: 4.9514\n",
      "Epoch 445/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0752\n",
      "Epoch 00445: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0783 - val_loss: 4.8873\n",
      "Epoch 446/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0516\n",
      "Epoch 00446: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0504 - val_loss: 4.8648\n",
      "Epoch 447/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0703\n",
      "Epoch 00447: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0715 - val_loss: 4.9087\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0764\n",
      "Epoch 00448: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0764 - val_loss: 5.1116\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0951\n",
      "Epoch 00449: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0951 - val_loss: 5.0402\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0954\n",
      "Epoch 00450: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0954 - val_loss: 4.8944\n",
      "Epoch 451/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0692\n",
      "Epoch 00451: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0688 - val_loss: 4.8390\n",
      "Epoch 452/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0852\n",
      "Epoch 00452: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0844 - val_loss: 4.8264\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0707\n",
      "Epoch 00453: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0707 - val_loss: 4.9513\n",
      "Epoch 454/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.1018\n",
      "Epoch 00454: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0995 - val_loss: 4.8401\n",
      "Epoch 455/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0829\n",
      "Epoch 00455: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0876 - val_loss: 4.8194\n",
      "Epoch 456/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0843\n",
      "Epoch 00456: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0852 - val_loss: 4.8498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0762\n",
      "Epoch 00457: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0774 - val_loss: 4.8035\n",
      "Epoch 458/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0748\n",
      "Epoch 00458: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0726 - val_loss: 4.8798\n",
      "Epoch 459/500\n",
      "14/16 [=========================>....] - ETA: 0s - loss: 0.0664\n",
      "Epoch 00459: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0678 - val_loss: 5.0986\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0857\n",
      "Epoch 00460: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0857 - val_loss: 5.0925\n",
      "Epoch 461/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0719\n",
      "Epoch 00461: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0743 - val_loss: 5.2588\n",
      "Epoch 462/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0895\n",
      "Epoch 00462: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0855 - val_loss: 5.1227\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0655\n",
      "Epoch 00463: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0655 - val_loss: 5.1728\n",
      "Epoch 464/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0774\n",
      "Epoch 00464: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0758 - val_loss: 5.1877\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 00465: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 0.0995 - val_loss: 5.0313\n",
      "Epoch 466/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0945\n",
      "Epoch 00466: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0956 - val_loss: 4.9094\n",
      "Epoch 467/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0793\n",
      "Epoch 00467: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0813 - val_loss: 4.9262\n",
      "Epoch 468/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0764\n",
      "Epoch 00468: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0752 - val_loss: 4.9059\n",
      "Epoch 469/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0727\n",
      "Epoch 00469: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0768 - val_loss: 5.0516\n",
      "Epoch 470/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0749\n",
      "Epoch 00470: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0763 - val_loss: 4.9076\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0700\n",
      "Epoch 00471: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0700 - val_loss: 5.0877\n",
      "Epoch 472/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0628\n",
      "Epoch 00472: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0627 - val_loss: 5.0252\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0766\n",
      "Epoch 00473: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0766 - val_loss: 5.1686\n",
      "Epoch 474/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0702\n",
      "Epoch 00474: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0717 - val_loss: 5.0830\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0654\n",
      "Epoch 00475: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0654 - val_loss: 4.9781\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 00476: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0768 - val_loss: 5.0860\n",
      "Epoch 477/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0724\n",
      "Epoch 00477: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 0.0710 - val_loss: 5.1287\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0650\n",
      "Epoch 00478: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0650 - val_loss: 5.0828\n",
      "Epoch 479/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0615\n",
      "Epoch 00479: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0604 - val_loss: 5.2634\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0644\n",
      "Epoch 00480: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0644 - val_loss: 5.2039\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0811\n",
      "Epoch 00481: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0811 - val_loss: 5.2576\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0664\n",
      "Epoch 00482: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0664 - val_loss: 5.2488\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0662\n",
      "Epoch 00483: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0662 - val_loss: 5.1251\n",
      "Epoch 484/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0649\n",
      "Epoch 00484: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0653 - val_loss: 5.1558\n",
      "Epoch 485/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0699\n",
      "Epoch 00485: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0693 - val_loss: 5.1820\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0818\n",
      "Epoch 00486: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0818 - val_loss: 5.1350\n",
      "Epoch 487/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0685\n",
      "Epoch 00487: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0678 - val_loss: 5.1411\n",
      "Epoch 488/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0677\n",
      "Epoch 00488: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0696 - val_loss: 5.3003\n",
      "Epoch 489/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0661\n",
      "Epoch 00489: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0651 - val_loss: 5.2125\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0689\n",
      "Epoch 00490: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0689 - val_loss: 5.0169\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 00491: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0978 - val_loss: 5.1812\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0712\n",
      "Epoch 00492: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0712 - val_loss: 5.0579\n",
      "Epoch 493/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0634\n",
      "Epoch 00493: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.0616 - val_loss: 5.0842\n",
      "Epoch 494/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0704\n",
      "Epoch 00494: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.0729 - val_loss: 5.0858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 495/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0729\n",
      "Epoch 00495: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0729 - val_loss: 5.0454\n",
      "Epoch 496/500\n",
      "15/16 [===========================>..] - ETA: 0s - loss: 0.0806\n",
      "Epoch 00496: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0802 - val_loss: 5.0407\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0802\n",
      "Epoch 00497: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0802 - val_loss: 5.1541\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0578\n",
      "Epoch 00498: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0578 - val_loss: 5.1858\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0735\n",
      "Epoch 00499: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0735 - val_loss: 5.1934\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0728\n",
      "Epoch 00500: val_loss did not improve from 2.19806\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.0728 - val_loss: 5.2620\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_train),np.array(y_train),batch_size=128,epochs=500, validation_data=(np.array(x_test),np.array(y_test)),verbose=1, callbacks=[mc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           5800      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 58)                14906     \n",
      "=================================================================\n",
      "Total params: 229,026\n",
      "Trainable params: 229,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('best_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 376
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 11, 12, 12, 7, 27, 7, 13, 12, 12, 12, 13, 27, 27, 27, 29, 25, 25, 6, 29]\n"
     ]
    }
   ],
   "source": [
    "# Generate music\n",
    "ind = np.random.randint(0,len(x_test)-1)\n",
    "\n",
    "random_music = x_test[ind]\n",
    "\n",
    "predictions=[]\n",
    "for i in range(20):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G4', '11.4', 'B-4', 'B-4', 'G4', 'E2', 'G4', 'F#4', 'B-4', 'B-4', 'B-4', 'F#4', 'E2', 'E2', 'E2', 'E5', 'G3', 'G3', 'C5', 'E5']\n"
     ]
    }
   ],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]\n",
    "print(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='gen3.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
